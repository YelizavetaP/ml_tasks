{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5630b0ca",
      "metadata": {
        "id": "5630b0ca"
      },
      "source": [
        "# Створення додатку з Retrieval Augmented Generation (RAG)\n",
        "\n",
        "Одним з найпотужніших застосувань, які забезпечують LLM, є складні чат-боти для запитань і відповідей (Q&A). Це програми, які можуть відповідати на запитання про конкретну інформацію з джерела користувача. Ці програми використовують техніку, відому як Retrieval Augmented Generation, або RAG.\n",
        "\n",
        "В цьому розділі ми подивимось як створити просту програму для Q&A на основі текстового джерела даних. Під час цього ми розглянемо типову архітектуру Q&A та в кінці я надам додаткові ресурси для більш просунутих технік Q&A. Ми також побачимо, як LangSmith може допомогти нам відстежувати та розуміти нашу програму. LangSmith стане все більш корисним, оскільки наша програма зростатиме в складності.\n",
        "\n",
        "## Що таке RAG?\n",
        "\n",
        "RAG — це техніка, що дозволяє доповнювати знання LLM додатковими даними.\n",
        "\n",
        "LLM можуть міркувати про широкий спектр тем, але їхні знання обмежені публічними даними до певного моменту часу, на яких вони були навчені (cutoff). Якщо ви хочете створити AI програми, які можуть враховувати приватні дані або дані, введені після дати відсічення моделі, вам потрібно доповнити знання моделі конкретною інформацією, яка їй потрібна. Процес внесення та вставки відповідної інформації в запит моделі відомий як Retrieval Augmented Generation (RAG).\n",
        "\n",
        "LangChain має ряд компонентів, призначених для допомоги у створенні програм Q&A, а також програм з використанням RAG.\n",
        "\n",
        "**Примітка**: Тут ми зосереджуємося на Q&A для неструктурованих даних. Якщо вас цікавить RAG для структурованих даних - можна ознайомитись з прикладом тут [питання і відповіді на SQL дані](https://python.langchain.com/docs/tutorials/sql_qa/).\n",
        "\n",
        "## Концепції\n",
        "Типова програма з використанням RAG має дві основні компоненти:\n",
        "\n",
        "**Індексація**: конвеєр для завантаження даних з джерела та їх індексації. *Це зазвичай відбувається офлайн. Тобто не прям коли ми викликаємо модель для відповідей на наші питання*\n",
        "\n",
        "**Пошук і генерація**: фактичний ланцюг RAG, який бере запит користувача під час виконання та отримує відповідні дані з індексу, а потім передає їх моделі.\n",
        "\n",
        "Найбільш поширена повна послідовність від сирих даних до відповіді виглядає так:\n",
        "\n",
        "### Індексація\n",
        "1. **Завантажити**: Спочатку потрібно завантажити наші дані. Це робиться за допомогою [Document Loaders](https://python.langchain.com/docs/concepts/document_loaders/).\n",
        "2. **Розділити**: [Text splitters](https://python.langchain.com/docs/concepts/text_splitters/) розбивають великі `Documents` на менші частини. Це корисно як для індексації даних, так і для передачі їх у модель, оскільки великі частини важче шукати і вони не помістяться в обмежене вікно контексту моделі.\n",
        "3. **Зберегти**: Нам потрібно місце для зберігання та індексації наших частин, щоб їх можна було шукати пізніше. Це часто робиться за допомогою [VectorStore](https://python.langchain.com/docs/concepts/vectorstores/) та [Embeddings](https://python.langchain.com/docs/concepts/embedding_models/) моделі.\n",
        "\n",
        "![index_diagram](https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png)\n",
        "\n",
        "### Пошук і генерація\n",
        "4. **Отримання**: З урахуванням введення користувача, відповідні частини отримуються зі сховища за допомогою [Retriever](https://python.langchain.com/docs/concepts/retrievers/).\n",
        "5. **Генерація**: [ChatModel](https://python.langchain.com/docs/concepts/chat_models/) / [LLM](https://python.langchain.com/docs/concepts/text_llms/) генерує відповідь, використовуючи запит, що включає як запит, так і отримані дані.\n",
        "\n",
        "![retrieval_diagram](https://python.langchain.com/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Встановлюємо залежності:"
      ],
      "metadata": {
        "id": "HvH_gRX547C3"
      },
      "id": "HvH_gRX547C3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1918ba2f",
      "metadata": {
        "id": "1918ba2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10c91b8f-4de8-453a-8092-624bf0ea5866"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.7/615.7 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet --upgrade langchain langchain-community langchain-chroma langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ff1b425",
      "metadata": {
        "id": "9ff1b425"
      },
      "source": [
        "### LangSmith\n",
        "\n",
        "Багато з додатків, які ви створюєте з LangChain, міститимуть кілька етапів з кількома викликами LLM (Large Language Model).\n",
        "Оскільки ці додатки стають все більш складними, стає важливим мати можливість перевірити, що саме відбувається всередині вашого ланцюга або агента.\n",
        "Найкращий спосіб зробити це - за допомогою [LangSmith](https://smith.langchain.com).\n",
        "\n",
        "LangSmith - це універсальна платформа для розробників для кожного етапу життєвого циклу додатків на основі LLM. LangSmith допомагає вам відстежувати та оцінювати ваші LLM-apps та інтелектуальних агентів, щоб допомогти вам перейти від прототипу до production.\n",
        "\n",
        "Після реєстрації за вказаним вище посиланням, переконайтеся, що ви налаштували свої змінні середовища для початку трейсингу:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
      ],
      "metadata": {
        "id": "2Y4QJGND5j15"
      },
      "id": "2Y4QJGND5j15",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "with open('creds.json') as file:\n",
        "  creds = json.load(file)\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = creds[\"OPENAI_API_KEY\"]\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = creds[\"LANGCHAIN_API_KEY\"]"
      ],
      "metadata": {
        "id": "4_ayjJzm6wIe"
      },
      "id": "4_ayjJzm6wIe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Попередній перегляд\n",
        "\n",
        "У цьому уроці ми створимо додаток, який відповідає на запитання про вміст веб-сайту. Конкретний веб-сайт, який ми будемо використовувати, - це [LLM Powered Autonomous\n",
        "Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) блог-пост\n",
        "Ліліан Венг.\n",
        "\n",
        "Ми можемо створити простий індексуючий конвеєр і RAG (Retrieval-Augmented Generation) ланцюг  за 20\n",
        "рядків коду:"
      ],
      "metadata": {
        "id": "mxIR9XCO5ioM"
      },
      "id": "mxIR9XCO5ioM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26ef9d35",
      "metadata": {
        "id": "26ef9d35"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6281ec7b",
      "metadata": {
        "id": "6281ec7b"
      },
      "outputs": [],
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Завантажуємо, розбиваємо на частини та індексуємо вміст блогу.\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Отримуємо та генеруємо, використовуючи відповідні фрагменти блогу.\n",
        "retriever = vectorstore.as_retriever()\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "XnL2Gchp76KB",
        "outputId": "a885fe80-af09-4a51-ca36-fa25cecb9458"
      },
      "id": "XnL2Gchp76KB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Task Decomposition is a process where complex tasks are broken down into smaller, simpler steps or subtasks. Techniques like Chain of Thought (CoT) and Tree of Thoughts are used to decompose tasks, guiding a model to think step by step, explore multiple reasoning possibilities, and thus transform big tasks into multiple manageable tasks. This process can be facilitated through different methods such as simple prompting, task-specific instructions, or human inputs.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d56d203",
      "metadata": {
        "id": "3d56d203"
      },
      "outputs": [],
      "source": [
        "# наводимо лад\n",
        "vectorstore.delete_collection()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9d51135",
      "metadata": {
        "id": "c9d51135"
      },
      "source": [
        "Перегляньте [LangSmith trace](https://smith.langchain.com/public/1c6ca97e-445b-4d00-84b4-c7befcbc59fe/r).\n",
        "\n",
        "## Детальний огляд\n",
        "\n",
        "Давайте пройдемося по наведеному коду крок за кроком, щоб дійсно зрозуміти, що відбувається.\n",
        "\n",
        "## 1. Індексація: Завантаження\n",
        "\n",
        "Спочатку нам потрібно завантажити вміст блогу. Для цього ми можемо використовувати\n",
        "DocumentLoaders,\n",
        "які є об'єктами, що завантажують дані з джерела і повертають список\n",
        "[Documents](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html).\n",
        "`Document` - це об'єкт з деяким `page_content` (str) та `metadata`\n",
        "(dict).\n",
        "\n",
        "У цьому випадку ми використаємо\n",
        "[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html),\n",
        "який використовує `urllib` для завантаження HTML з веб-URL і `BeautifulSoup` для\n",
        "перетворення його в текст. Ми можемо налаштувати перетворення HTML -> текст, передавши\n",
        "параметри в парсер `BeautifulSoup` через `bs_kwargs` (див.\n",
        "[документацію BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/#beautifulsoup)).\n",
        "\n",
        "У цьому випадку лише HTML-теги з класом “post-content”, “post-title” або\n",
        "“post-header” є релевантними, тому ми видалимо всі інші."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5ba0122-8c92-4895-b5ef-f03a634e3fdf",
      "metadata": {
        "id": "f5ba0122-8c92-4895-b5ef-f03a634e3fdf",
        "outputId": "ba3d875c-f228-4e43-94c2-f7ca6d046336",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43131"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# Лишаємо лише назву, заголовки та вміст допису з повного HTML.\n",
        "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "len(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cf74be6-5f40-4f6d-8689-b6b42ced8b70",
      "metadata": {
        "id": "5cf74be6-5f40-4f6d-8689-b6b42ced8b70",
        "outputId": "bfbcb1fd-1756-4cbf-eedb-aad498b72e23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "      LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "Agent System Overview#\n",
            "In\n"
          ]
        }
      ],
      "source": [
        "print(docs[0].page_content[:500])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2I7cTi9Mpc6",
        "outputId": "4fd42b9e-03e1-4516-89f0-eeaa4c421690"
      },
      "id": "f2I7cTi9Mpc6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07845e7a",
      "metadata": {
        "id": "07845e7a"
      },
      "source": [
        "### Заглиблення\n",
        "\n",
        "`DocumentLoader`: Об'єкт, який завантажує дані з джерела у вигляді списку `Documents`.\n",
        "\n",
        "- [Документація](https://python.langchain.com/docs/how_to/#document-loaders):\n",
        "  Докладна документація про те, як використовувати `DocumentLoaders`.\n",
        "- [Інтеграції](https://python.langchain.com/docs/integrations/document_loaders/): 160+\n",
        "  інтеграцій на вибір.\n",
        "- [Інтерфейс](https://python.langchain.com/api_reference/core/document_loaders/langchain_core.document_loaders.base.BaseLoader.html):\n",
        "  API-довідка для базового інтерфейсу.\n",
        "\n",
        "\n",
        "## 2. Індексація: Розділення\n",
        "\n",
        "\n",
        "Наш завантажений документ містить понад 42 тисячі символів, що занадто багато, щоб вміститися у вікно контексту багатьох моделей. Навіть для тих моделей, які можуть вмістити весь пост у своєму вікні контексту, моделі можуть мати труднощі з пошуком інформації у дуже довгих введеннях.\n",
        "\n",
        "Перевірити величину контекстного вікна в OpenAI моделей можна тут: https://platform.openai.com/docs/models\n",
        "\n",
        "Щоб впоратися з цим, ми розділимо `Document` на частини для вбудовування (embedding) та зберігання векторів. Це має допомогти нам отримувати лише найрелевантніші частини блогу під час обробки запиту від користувача.\n",
        "\n",
        "У цьому випадку ми розділимо наші документи на частини по 1000 символів з 200 символами перекриття між частинами. Перекриття допомагає зменшити ймовірність відокремлення твердження від важливого контексту, пов'язаного з ним. Ми використовуємо\n",
        "[RecursiveCharacterTextSplitter](https://python.langchain.com/docs/how_to/recursive_text_splitter/),\n",
        "який рекурсивно розділяє документ, використовуючи загальні роздільники, такі як нові рядки, поки кожна частина не досягне відповідного розміру. Це рекомендований роздільник тексту для загальних випадків використання тексту.\n",
        "\n",
        "Ми встановлюємо `add_start_index=True`, щоб індекс символів, з якого починається кожен розділений документ у початковому документі, зберігався як атрибут метаданих \"start_index\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aa3f8c0-5113-4c36-9706-ee702407173a",
      "metadata": {
        "id": "6aa3f8c0-5113-4c36-9706-ee702407173a",
        "outputId": "cda60762-a105-4a30-91a1-4711ffcefe5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
        ")\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "len(all_splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2257752c-bed2-4d57-be8e-d275bfe70ace",
      "metadata": {
        "id": "2257752c-bed2-4d57-be8e-d275bfe70ace",
        "outputId": "7ae45d3e-4e0b-405d-cade-6a42055d640b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "969"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "len(all_splits[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_splits[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdW3PbNpNY-h",
        "outputId": "b820be57-d23a-42a1-cc00-30236246b2f1"
      },
      "id": "bdW3PbNpNY-h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 8}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "325fdc48-4a24-4645-9d08-0d22f5be5e13",
      "metadata": {
        "id": "325fdc48-4a24-4645-9d08-0d22f5be5e13",
        "outputId": "42d768e8-7820-42cb-b738-9f3115f3aee6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
              " 'start_index': 7056}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_splits[10].metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7046d580",
      "metadata": {
        "id": "7046d580"
      },
      "source": [
        "### Заглиблення\n",
        "\n",
        "`TextSplitter`: Об'єкт, який розділяє список `Document` на менші частини. Підклас `DocumentTransformer`.\n",
        "\n",
        "- Дізнатись більше про різні методи розділення тексту ви можете прочитавши [документацію](https://python.langchain.com/docs/how_to/#text-splitters)\n",
        "- [Код (py або js)](https://python.langchain.com/docs/integrations/document_loaders/source_code/)\n",
        "- [Наукові статті](https://python.langchain.com/docs/integrations/document_loaders/grobid/)\n",
        "- [Інтерфейс](https://python.langchain.com/api_reference/text_splitters/base/langchain_text_splitters.base.TextSplitter.html): API-довідка для базового інтерфейсу.\n",
        "\n",
        "`DocumentTransformer`: Об'єкт, який виконує трансформацію над списком об'єктів `Document`.\n",
        "\n",
        "- [Документація](https://python.langchain.com/docs/how_to/#text-splitters): Докладна документація про те, як використовувати `DocumentTransformers`\n",
        "- [Інтеграції](https://python.langchain.com/docs/integrations/document_transformers/)\n",
        "- [Інтерфейс](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.transformers.BaseDocumentTransformer.html): API-довідка для базового інтерфейсу.\n",
        "\n",
        "## 3. Індексація: Зберігання\n",
        "\n",
        "Тепер нам потрібно проіндексувати наші 66 текстових частин, щоб ми могли шукати їх під час виконання.\n",
        "\n",
        "Найпоширеніший спосіб зробити це — вбудувати вміст кожного розділеного документа і вставити ці ембедінги в векторну базу даних (або векторне сховище). Коли ми хочемо шукати серед наших розділів, ми беремо текстовий запит на пошук, вбудовуємо його і виконуємо певний вид пошуку на \"схожість\", щоб визначити збережені розділи з найбільш схожими ембедінгами на наш ембедінг запиту. Найпростіша міра схожості — косинусна схожість — ми вимірюємо косинус кута між кожною парою ембедінгів (які є векторами високої розмірності).\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:1400/1*-AL-kK8HzK5lw84xr1cSvw.png)\n",
        "\n",
        "Ми можемо вбудувати і зберегти всі наші розділи документів одною командою, використовуючи векторне сховище [Chroma](/https://python.langchain.com/docs/integrations/vectorstores/chroma/) та модель [OpenAIEmbeddings](https://python.langchain.com/docs/integrations/text_embedding/openai/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b44b41a-8b25-42ad-9e37-7baf82a058cd",
      "metadata": {
        "id": "0b44b41a-8b25-42ad-9e37-7baf82a058cd"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9vVSIUBN4cC",
        "outputId": "f02e9d51-421f-45b0-8413-b624ce924efc"
      },
      "id": "f9vVSIUBN4cC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_chroma.vectorstores.Chroma at 0x7a54e95b01c0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbddc12e",
      "metadata": {
        "id": "dbddc12e"
      },
      "source": [
        "### Заглиблення\n",
        "\n",
        "`Embeddings`: Обгортка навколо моделі векторизації тексту, що використовується для перетворення тексту на вектори.\n",
        "\n",
        "- [Документація](https://python.langchain.com/docs/how_to/embed_text/): Докладна документація про те, як використовувати вектори.\n",
        "- [Інтеграції](https://python.langchain.com/docs/integrations/text_embedding/): Понад 30 інтеграцій на вибір.\n",
        "- [Інтерфейс](https://python.langchain.com/api_reference/core/embeddings/langchain_core.embeddings.Embeddings.html): API-довідка для базового інтерфейсу.\n",
        "\n",
        "`VectorStore`: Обгортка навколо векторної бази даних, що використовується для зберігання та запитів до векторів. Векторн база зберігає вектори ефективніше за звичайну.\n",
        "\n",
        "- [Документація](https://python.langchain.com/docs/how_to/vectorstores/): Докладна документація про те, як використовувати векторні сховища.\n",
        "- [Інтеграції](https://python.langchain.com/docs/integrations/vectorstores/): Понад 40 інтеграцій на вибір.\n",
        "- [Інтерфейс](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.VectorStore.html): API-довідка для базового інтерфейсу.\n",
        "\n",
        "Це завершує частину **Індексації** в конвеєрі. На цьому етапі у нас є векторне сховище, що містить розділені частини нашого блогу, до якого можна робити запити. Відповідаючи на запит користувача, ми повинні мати можливість повернути фрагменти блогу, які відповідають на запит.\n",
        "\n",
        "## 4. Retrieval & Generation: Retrieve\n",
        "\n",
        "Тепер давайте напишемо фактичну логіку програми. Ми хочемо створити просту програму, яка приймає запит користувача, шукає документи, що стосуються цього запиту, передає знайдені документи та початкове питання моделі і повертає відповідь.\n",
        "\n",
        "Спочатку нам потрібно визначити нашу логіку для пошуку документів. LangChain визначає\n",
        "[Retriever](https://python.langchain.com/docs/concepts/#retrievers/) інтерфейс,\n",
        "який обгортає індекс, що може повертати відповідні `Documents` за заданим рядковим запитом.\n",
        "\n",
        "Найпоширенішим типом `Retriever` є\n",
        "[VectorStoreRetriever](https://python.langchain.com/docs/how_to/vectorstore_retriever/),\n",
        "який використовує можливості пошуку подібності векторного сховища для полегшення витягування потрібних документів. Будь-який `VectorStore` можна легко перетворити на `Retriever` за допомогою `VectorStore.as_retriever()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a0d25f8-8a45-4ec7-b419-c36e231fde13",
      "metadata": {
        "id": "1a0d25f8-8a45-4ec7-b419-c36e231fde13",
        "outputId": "07ae4296-41dd-438d-b0a2-8fc1c26034bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
        "\n",
        "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
        "\n",
        "len(retrieved_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58db0a6a-f1ad-4d28-acf8-98be9ed3c968",
      "metadata": {
        "id": "58db0a6a-f1ad-4d28-acf8-98be9ed3c968",
        "outputId": "19533581-be0e-462c-e2b0-88ec700cf607",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
            "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n"
          ]
        }
      ],
      "source": [
        "print(retrieved_docs[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(retrieved_docs[-1].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GB_gl1LCRCjC",
        "outputId": "4c0756a1-6068-4716-ddc2-92667589116e"
      },
      "id": "GB_gl1LCRCjC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
            "Self-Reflection#\n",
            "Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bb602b0",
      "metadata": {
        "id": "8bb602b0"
      },
      "source": [
        "### Заглиблення\n",
        "\n",
        "Векторні сховища зазвичай використовуються для отримання інформації, але існують й інші способи виконання запитів.\n",
        "\n",
        "`Retriever`: Об'єкт, який повертає `Document` (документи) за текстовим запитом.\n",
        "\n",
        "- [Документація](https://python.langchain.com/docs/how_to/#retrievers): Додаткова документація про інтерфейс та вбудовані техніки отримання інформації. Деякі з них включають:\n",
        "  - `MultiQueryRetriever` [генерує варіанти вхідного запитання](https://python.langchain.com/docs/how_to/MultiQueryRetriever) для покращення retrieval hit rate (кількості влучно отриманих чанків документів).\n",
        "  - `MultiVectorRetriever` натомість генерує [варіанти векторів](https://python.langchain.com/docs/how_to/multi_vector), також з метою покращення retrieval hit rate..\n",
        "  - `Maximal marginal relevance` максимізує [релевантність та різноманітність](https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf) серед отриманих документів, щоб уникнути повторення контексту.\n",
        "  - Документи можуть бути відфільтровані під час отримання інформації з векторного сховища за допомогою фільтрів метаданих, таких як у [Self Query Retriever](https://python.langchain.com/docs/how_to/self_query).\n",
        "- [Інтеграції](https://python.langchain.com/docs/integrations/retrievers/): Інтеграції з сервісами отримання інформації.\n",
        "- [Інтерфейс](https://python.langchain.com/api_reference/core/retrievers/langchain_core.retrievers.BaseRetriever.html): API-довідка для базового інтерфейсу.\n",
        "\n",
        "## 5. Retrieval & Generation: Generate\n",
        "\n",
        "Давайте об'єднаємо все в ланцюг, який приймає запитання, отримує відповідні документи, формує запит, передає його в модель і аналізує вихідні дані.\n",
        "\n",
        "Ми використаємо модель чату gpt-4o-mini від OpenAI, але могли б використати будь-яку `LLM` або `ChatModel` з тих, що дає можливість використовувати LangChain.\n",
        "\n",
        "Ми використаємо запит для RAG, який зберігається в хабі запитів LangChain ([тут](https://smith.langchain.com/hub/rlm/rag-prompt))."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")"
      ],
      "metadata": {
        "id": "2pwUUNA2CrhF"
      },
      "id": "2pwUUNA2CrhF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt.messages[0].prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxSpvVmdCshn",
        "outputId": "759b79e4-a271-46b1-e835-82ec0510eb81"
      },
      "id": "VxSpvVmdCshn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
            "Question: {question} \n",
            "Context: {context} \n",
            "Answer:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff01d415-7b0f-469d-bfda-b9cb672da611",
      "metadata": {
        "id": "ff01d415-7b0f-469d-bfda-b9cb672da611",
        "outputId": "fba112d6-b84b-41a1-d2ca-e5ba8719b24c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: my unique question \\nContext: my interesting context \\nAnswer:\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "example_messages = prompt.invoke(\n",
        "    {\"context\": \"my interesting context\", \"question\": \"my unique question\"}\n",
        ").to_messages()\n",
        "\n",
        "example_messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2885ed99-31a0-4d7e-b9b0-af49c462caf4",
      "metadata": {
        "id": "2885ed99-31a0-4d7e-b9b0-af49c462caf4",
        "outputId": "3c8c6074-e7be-4e04-8e79-d210c4a032a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
            "Question: my unique question \n",
            "Context: my interesting context \n",
            "Answer:\n"
          ]
        }
      ],
      "source": [
        "print(example_messages[0].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4516200c",
      "metadata": {
        "id": "4516200c"
      },
      "source": [
        "Ми використаємо протокол [LCEL Runnable](https://python.langchain.com/docs/concepts#langchain-expression-language-lcel) для визначення ланцюга, що дозволить нам\n",
        "\n",
        "- з'єднувати компоненти та функції прозорим способом\n",
        "- автоматично відстежувати наш ланцюг у LangSmith\n",
        "- отримувати потокове, асинхронне та пакетне викликання з коробки.\n",
        "\n",
        "Ось реалізація:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6820cf3-e14d-4275-bd00-aa1b8262b1ae",
      "metadata": {
        "id": "d6820cf3-e14d-4275-bd00-aa1b8262b1ae",
        "outputId": "f1fff36b-2bc4-4ac2-a293-844839b7d891",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task Decomposition is the process of breaking down complex tasks into smaller, simpler steps. This can be achieved through techniques like Chain of Thought and Tree of Thoughts, which transform large assignments into multiple manageable tasks. Decomposition can be guided by simple prompting, task-specific instructions, or human inputs."
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dacf214-0803-46f1-960d-42336a545e39",
      "metadata": {
        "id": "3dacf214-0803-46f1-960d-42336a545e39"
      },
      "source": [
        "Давайте розглянемо LCEL, щоб зрозуміти, що відбувається.\n",
        "\n",
        "По-перше: кожен з цих компонентів (`retriever`, `prompt`, `llm` тощо) є екземплярами [Runnable](https://python.langchain.com/docs/concepts#langchain-expression-language-lcel). Це означає, що вони реалізують однакові методи - такі як синхронний і асинхронний `.invoke`, `.stream` або `.batch` - що полегшує їх з'єднання. Їх можна з'єднувати в [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html) - ще один Runnable - за допомогою оператора `|`.\n",
        "\n",
        "LangChain автоматично перетворює певні об'єкти на runnable, коли стикається з оператором `|`. Тут `format_docs` перетворюється на [RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html), а словник з `\"context\"` і `\"question\"` перетворюється на [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html). Деталі менш важливі, ніж загальна ідея, яка полягає в тому, що кожен об'єкт у ланцюзі є Runnable.\n",
        "\n",
        "Давайте прослідкуємо, як вхідне питання проходить через вищезгадані runnable.\n",
        "\n",
        "Як ми бачили вище, вхід для `prompt` очікується у вигляді словника з ключами `\"context\"` і `\"question\"`. Отже, перший елемент цього ланцюга створює runnable, які обчислять обидва ці значення з вхідного питання:\n",
        "- `retriever | format_docs` передає питання через retriever, генеруючи [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) об'єкти, а потім до `format_docs`, щоб згенерувати рядки;\n",
        "- `RunnablePassthrough()` передає вхідне питання без змін.\n",
        "\n",
        "Тобто, якщо ви створите\n",
        "```python\n",
        "chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        ")\n",
        "```\n",
        "Тоді `chain.invoke(question)` створить відформатований запит, готовий для інференції. (Примітка: під час розробки з LCEL може бути практично тестувати з підланцюгами, як цей.)\n",
        "\n",
        "Останні кроки ланцюга - це `llm`, який виконує інференцію, і `StrOutputParser()`, який просто витягує рядковий вміст з виходу LLM.\n",
        "\n",
        "Ви можете проаналізувати окремі кроки цього ланцюга через його [LangSmith trace](https://smith.langchain.com/public/1799e8db-8a6d-4eb2-84d5-46e8d7d5a99b/r).\n",
        "\n",
        "### Вбудовані ланцюги (з базового функціоналу LangChain)\n",
        "\n",
        "Якщо потрібно, LangChain включає зручні функції, які реалізують вищезгаданий LCEL. Ми складаємо дві функції:\n",
        "\n",
        "- [create_stuff_documents_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html) визначає, як отриманий контекст подається в запит і LLM. У цьому випадку ми \"заповнимо\" вміст у запит - тобто, ми включимо весь отриманий контекст без будь-якого узагальнення або іншої обробки. Це в основному реалізує наш вище `rag_chain`, з вхідними ключами `context` і `input` - він генерує відповідь, використовуючи отриманий контекст і запит.\n",
        "- [create_retrieval_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.retrieval.create_retrieval_chain.html) додає крок отримання (retrieval) і передає отриманий контекст через ланцюг, надаючи його разом з фінальною відповіддю. Він має вхідний ключ `input` і включає `input`, `context` і `answer` у своєму виході."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e75bfe98-d9e4-4868-bae1-5811437d859b",
      "metadata": {
        "id": "e75bfe98-d9e4-4868-bae1-5811437d859b",
        "outputId": "95e65ce8-0494-40bb-99f4-ff6254be4dc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task Decomposition is the process of breaking down a complex task into smaller, simpler steps. Techniques such as Chain of Thought (CoT) and Tree of Thoughts are used to enhance model performance on complex tasks. They instruct the model to think step by step, and decompose hard tasks into manageable ones, thus providing an interpretation of the model’s thinking process.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are an assistant for question-answering tasks. \"\n",
        "    \"Use the following pieces of retrieved context to answer \"\n",
        "    \"the question. If you don't know the answer, say that you \"\n",
        "    \"don't know. Use three sentences maximum and keep the \"\n",
        "    \"answer concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "\n",
        "response = rag_chain.invoke({\"input\": \"What is Task Decomposition?\"})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fe711ea-592b-44a1-89b3-cee33c81aca4",
      "metadata": {
        "id": "0fe711ea-592b-44a1-89b3-cee33c81aca4"
      },
      "source": [
        "#### Одержання джерел\n",
        "Часто в Q&A додатках важливо показувати користувачам джерела, які були використані для генерації відповіді. Вбудована функція LangChain `create_retrieval_chain` передасть отримані джерела документів у вихідні дані під ключем `\"context\"`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d4cec1a-75d6-4479-929f-72cadb2dcde8",
      "metadata": {
        "id": "9d4cec1a-75d6-4479-929f-72cadb2dcde8",
        "outputId": "7c37d636-f131-4630-8175-54e4ed7b408e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
            "Component One: Planning#\n",
            "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
            "Task Decomposition#\n",
            "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}\n",
            "\n",
            "page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
            "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}\n",
            "\n",
            "page_content='Resources:\n",
            "1. Internet access for searches and information gathering.\n",
            "2. Long Term memory management.\n",
            "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
            "4. File output.\n",
            "\n",
            "Performance Evaluation:\n",
            "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n",
            "2. Constructively self-criticize your big-picture behavior constantly.\n",
            "3. Reflect on past decisions and strategies to refine your approach.\n",
            "4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29630}\n",
            "\n",
            "page_content='(3) Task execution: Expert models execute on the specific tasks and log results.\n",
            "Instruction:\n",
            "\n",
            "With the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19373}\n",
            "\n",
            "page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17804}\n",
            "\n",
            "page_content='Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\n",
            "The system comprises of 4 stages:\n",
            "(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\n",
            "Instruction:' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17414}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for document in response[\"context\"]:\n",
        "    print(document)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cd57618",
      "metadata": {
        "id": "7cd57618"
      },
      "source": [
        "### Заглиблення\n",
        "\n",
        "#### Вибір моделі\n",
        "\n",
        "`ChatModel`: Модель чату на основі LLM. Приймає послідовність повідомлень і повертає повідомлення.\n",
        "\n",
        "- [Документація](https://python.langchain.com/docs/how_to#chat-models)\n",
        "- [Інтеграції](https://python.langchain.com/docs/integrations/chat/): більше 25 інтеграцій на вибір.\n",
        "- [Інтерфейс](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html): API-довідка для базового інтерфейсу.\n",
        "\n",
        "`LLM`: Модель LLM з текстом на вході та текстом на виході. Приймає рядок і повертає рядок.\n",
        "\n",
        "- [Документація](https://python.langchain.com/docs/how_to#llms)\n",
        "- [Інтеграції](https://python.langchain.com/docs/integrations/llms): більше 75 інтеграцій на вибір.\n",
        "- [Інтерфейс](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.llms.BaseLLM.html): API-довідка для базового інтерфейсу.\n",
        "\n",
        "Дивіться посібник з RAG з локально запущеними моделями [тут](https://python.langchain.com/docs/tutorials/local_rag).\n",
        "\n",
        "#### Налаштування запиту\n",
        "\n",
        "Як показано вище, ми можемо завантажувати запити (наприклад, [цей запит RAG](https://smith.langchain.com/hub/rlm/rag-prompt)) з хабу запитів. Запит також можна легко налаштувати:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ac552b6",
      "metadata": {
        "id": "2ac552b6",
        "outputId": "0a9cccb0-8edf-4027-a03b-14df683c37e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Task decomposition is a process in which a complex task is broken down into smaller, simpler steps or subtasks. Techniques like Chain of Thought (CoT) and Tree of Thoughts help in this process by systematically decomposing tasks and exploring multiple reasoning possibilities. This allows an autonomous agent to manage and execute larger tasks more efficiently. What other aspects of the LLM-powered autonomous agent system would you like to know about?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "Always ask next question at the end to clarify what else user might be interested in.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\"\"\"\n",
        "custom_rag_prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | custom_rag_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82e4d779",
      "metadata": {
        "id": "82e4d779"
      },
      "source": [
        "Перегляньте [LangSmith trace](https://smith.langchain.com/public/da23c4d8-3b33-47fd-84df-a3a582eedf84/r)\n",
        "\n",
        "## Наступні кроки\n",
        "\n",
        "Ми розглянули етапи створення базового додатку для запитань і відповідей (Q&A) на основі даних:\n",
        "\n",
        "- Завантаження даних за допомогою [Document Loader](https://python.langchain.com/docs/concepts#document-loaders)\n",
        "- Розбиття індексованих даних за допомогою [Text Splitter](https://python.langchain.com/docs/concepts#text-splitters) для зручнішого використання моделлю\n",
        "- [Вбудовування даних](https://python.langchain.com/docs/concepts#embedding-models) та зберігання даних у [vectorstore](https://python.langchain.com/docs/how_to/vectorstores)\n",
        "- [Отримання](https://python.langchain.com/docs/concepts#retrievers) раніше збережених частин у відповідь на вхідні запитання\n",
        "- Генерація відповіді, використовуючи отримані частини як контекст\n",
        "\n",
        "Є безліч функцій, інтеграцій та розширень, які можна дослідити в кожному з вищезазначених розділів. Разом з **Go deeper** джерелами, згаданими вище, хорошими наступними кроками є:\n",
        "\n",
        "- [Повернення джерел](https://python.langchain.com/docs/how_to/qa_sources): Дізнайтеся, як повертати джерела документів\n",
        "- [Стрімінг](https://python.langchain.com/docs/how_to/streaming): Дізнайтеся, як стрімити виходи та проміжні етапи\n",
        "- [Додати історію чату](https://python.langchain.com/docs/how_to/message_history): Дізнайтеся, як додати історію чату до вашого додатку\n",
        "- [Концептуальний посібник з отримання](https://python.langchain.com/docs/concepts/retrieval): Загальний огляд специфічних технік отримання (retrieval)\n",
        "- [Створити локальний RAG додаток](https://python.langchain.com/docs/tutorials/local_rag): Створіть додаток, подібний до наведеного вище, використовуючи всі локальні компоненти"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}