{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CONjBYGh0bY"
      },
      "source": [
        "# Основи роботи з LangChain\n",
        "\n",
        "LangChain — це фреймворк, створений для полегшення побудови застосунків на основі великих мовних моделей (LLMs). Основною ідеєю LangChain є інтеграція LLM у більш складні та персоналізовані робочі процеси, що дозволяє використовувати мовні моделі не просто для генерації тексту, але і для виконання складних бізнес-завдань або автоматизації процесів.\n",
        "\n",
        "LangChain дозволяє поєднувати LLM з зовнішніми інструментами, базами даних або API. Завдяки цьому можна створювати складніші сценарії використання, наприклад:\n",
        "\n",
        "1. **Інтеграція з базами даних**: LangChain може працювати з великими обсягами даних або підключатися до зовнішніх джерел інформації, надаючи змогу LLM доступати до актуальних або специфічних даних.\n",
        "  \n",
        "2. **Запам'ятовування контексту**: Зазвичай LLM не має \"пам'яті\", але LangChain дозволяє зберігати стан і контекст між запитами, що корисно для чат-ботів або додатків, які вимагають пам’яті про попередні запити або сесії, однак LangChain може допомогти реалізувати механізми збереження контексту розмови або дій. Це дозволяє моделі пам’ятати попередні відповіді або завдання, роблячи роботу з LLM більш цілісною і логічною в довгих сесіях.\n",
        "\n",
        "3. **Розгалужені ланцюги дій**: За допомогою LangChain можна побудувати ланцюжки дій, де LLM виконує серію кроків або запитів, заснованих на попередніх результатах. Це корисно для складних задач, які вимагають кількох етапів обробки даних або прийняття рішень.\n",
        "\n",
        "4. **Інтеграція з іншими інструментами**: LangChain дозволяє легко підключати сторонні інструменти, такі як інтерфейси користувача, системи аналітики або автоматизації бізнес-процесів, що розширює можливості моделі.\n",
        "\n",
        "5. **Індивідуалізація застосунків**: Розробники можуть створювати спеціалізовані програми для виконання певних завдань на основі даних або бізнес-потреб, таких як чат-боти, автоматизовані системи підтримки клієнтів або моделі рекомендацій.\n",
        "\n",
        "Таким чином, **LangChain** — це інструмент, який робить LLM більш гнучкими та функціональними в реальних умовах, дозволяючи будувати застосунки на основі штучного інтелекту з урахуванням складних сценаріїв і бізнес-потреб."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbYVgIZchwfq"
      },
      "source": [
        "Встановлення всіх необхідних пакетів..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRYSu48huSUW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d16d044b-a024-4a9c-bfca-8cc564dc0755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/75.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain langchain_openai huggingface_hub openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUaTbXu5vduH",
        "outputId": "31c54cea-369c-438b-812c-2c5e672d6b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.3.27\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6yiwXNnvzxO"
      },
      "source": [
        "#### Налаштування моделі OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2PbNnQhqzAL"
      },
      "source": [
        "Нам потрібно завантажити деякі токени. Я помістила їх у файл наступної структури:\n",
        "```\n",
        "{\n",
        "\"OPENAI_API_KEY\": \"...\",\n",
        "\"HUGGINGFACEHUB_API_TOKEN\": \"...\",\n",
        "\"SERPAPI_API_KEY\":\"...\"\n",
        "}\n",
        "\n",
        "```\n",
        "Потім я просто додала цей файл до контексту цього ноутбука."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1rye0_OetAU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "with open('creds.json') as file:\n",
        "  creds = json.load(file)\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = creds[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaabjHKmh7_m"
      },
      "source": [
        "Для більшості LLM (великі мовні моделі) ми можемо налаштувати параметр температури, який контролює креативність тексту, що генерується API OpenAI. Вища температура призведе до більш креативного тексту, тоді як нижча температура створить більш передбачуваний текст.\n",
        "\n",
        "Давайте зробимо наші виходи досить передбачуваними, але з невеликою часткою креативності.\n",
        "\n",
        "Значення за замовчуванням для температури зазвичай становить 0.7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mu6rG38mhFYo"
      },
      "outputs": [],
      "source": [
        "overal_temperature = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wYGExb_ioxi"
      },
      "source": [
        "Ви можете знайти опис параметрів моделей OpenAI [тут](https://python.langchain.com/docs/concepts/#chat-models).\n",
        "\n",
        "А загальну інформацію щодо моделей OpenAI з інтеграцією LangChain - [тут](https://python.langchain.com/docs/integrations/llms/openai)\n",
        "\n",
        "Сорс код класа `ChatOpenAI`: [тут](https://github.com/langchain-ai/langchain/blob/0640cbf2f126f773b7ae78b0f94c1ba0caabb2c1/libs/community/langchain_community/chat_models/openai.py#L180)\n",
        "\n",
        "## Різні OpenAI моделі\n",
        "\n",
        "У фреймворку **LangChain** існують дві основні класи для роботи з моделями OpenAI: **ChatOpenAI** та **OpenAI**. Ось їх основні відмінності:\n",
        "\n",
        "### 1. **OpenAI** (клас для роботи з мовними моделями):\n",
        "- **OpenAI** використовується для взаємодії з текстовими мовними моделями, такими як GPT-3 або GPT-4, які приймають текст як вхід і генерують текст у відповідь.\n",
        "- Цей клас добре підходить для виконання стандартних запитів, де ви передаєте текст і отримуєте текстовий результат.\n",
        "- **OpenAI** не підтримує багаторівневу структуру розмови чи контекстний діалог; це просто запит-відповідь без збереження історії діалогу.\n",
        "\n",
        "\n",
        "### 2. **ChatOpenAI** (клас для роботи з чат-моделями):\n",
        "- **ChatOpenAI** використовується для роботи з моделями, оптимізованими для чат-сценаріїв, такими як GPT-4, які можуть вести розмови та обробляти кілька повідомлень з діалогу.\n",
        "- Цей клас дозволяє працювати з історією чату, надаючи можливість передавати повідомлення як частину розмови, включаючи повідомлення від користувача (Human) і відповіді моделі (AI).\n",
        "- **ChatOpenAI** також краще підходить для додатків, де важливо зберігати контекст діалогу та попередні повідомлення.\n",
        "\n",
        "Таким чином, **ChatOpenAI** краще використовувати, коли потрібно будувати чат-ботів або вести розмову, тоді як **OpenAI** підходить для одноразових генерацій тексту на основі вхідного запиту."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lzO5PfUpwfv"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=overal_temperature)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "id": "DU7f8C6ovNcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "631511be-3917-4d8a-cdd5-f6c60e8fbc0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OpenAI(client=<openai.resources.completions.Completions object at 0x7bb059ad39b0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7bb05818fb60>, temperature=0.1, model_kwargs={}, openai_api_key=SecretStr('**********'))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMaNWpGMjhc7"
      },
      "source": [
        "### Отримання прогнозів"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI_Ve5bjjA2D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "936f64d8-89cc-42bd-e1b3-36cd980f3790"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nYou are talking to a computer program designed to respond to your questions and prompts.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "llm.invoke('who am I talking to?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqF-YPVEi7jZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8c4adad-f444-4e1a-860b-0889977f6bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Italy - The birthplace of pasta, Italy offers a wide variety of pasta dishes from different regions such as spaghetti carbonara, lasagna, and gnocchi.\n",
            "\n",
            "2. New York City, USA - Known for its diverse food scene, New York City has a plethora of Italian restaurants offering delicious pasta dishes such as fettuccine alfredo and penne arrabiata.\n",
            "\n",
            "3. Paris, France - While known for its French cuisine, Paris also has a strong Italian influence and offers a range of pasta dishes such as tagliatelle bolognese and linguine with clams.\n",
            "\n",
            "4. Tokyo, Japan - Surprisingly, Tokyo has a thriving Italian food scene and offers unique fusion dishes such as ramen carbonara and udon bolognese.\n",
            "\n",
            "5. Buenos Aires, Argentina - With a large Italian immigrant population, Buenos Aires has a strong Italian food culture and offers delicious pasta dishes such as ravioli and spaghetti with meatballs.\n"
          ]
        }
      ],
      "source": [
        "request = \"What are 5 vacation destinations for someone who likes to eat pasta?\"\n",
        "print(llm.invoke(request))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "А ось таким чином можна реалізувати стрімінг - це якщо ви захочете написати свого чатбота:"
      ],
      "metadata": {
        "id": "dsvW_bewZGfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in llm.stream(\n",
        "    \"What are some theories about the relationship between unemployment and inflation?\"\n",
        "):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "Ao3WkMGrZD4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Більше про різні інтерфеси як ранити LLM з langchain - [тут](https://python.langchain.com/docs/integrations/llms/openai/)"
      ],
      "metadata": {
        "id": "z2jH0-kqZNGE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulXC2DyLjtw7"
      },
      "source": [
        "### Промпти: Керування запитами для LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MP2yGFHej1Qr"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPq0lUpoj33z"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"food\"],\n",
        "    template=\"What are 5 vacation destinations for someone who likes to eat {food}?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPTvCstej7WP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "933987e2-4ff8-489d-ac53-d43d89b40d37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What are 5 vacation destinations for someone who likes to eat donuts?\n"
          ]
        }
      ],
      "source": [
        "print(prompt.format(food=\"donuts\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qA1FWh13j-uH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99b8d2cb-151c-4c80-84ba-8cdfd58ef5a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Portland, Oregon: Known for its vibrant food scene, Portland is home to numerous artisanal donut shops such as Voodoo Doughnut and Blue Star Donuts.\n",
            "\n",
            "2. New York City, New York: The Big Apple is a haven for foodies, and donut lovers will not be disappointed with the variety of options available, from classic New York-style bagels to trendy gourmet donut shops like Doughnut Plant.\n",
            "\n",
            "3. Austin, Texas: This hip and eclectic city is home to unique and creative donut shops like Gourdough's and Voodoo Doughnut, as well as traditional Texas-style kolaches.\n",
            "\n",
            "4. Nashville, Tennessee: In addition to its famous hot chicken, Nashville also boasts a thriving donut scene with popular spots like Five Daughters Bakery and Fox's Donut Den.\n",
            "\n",
            "5. Chicago, Illinois: The Windy City is known for its deep-dish pizza, but it also has a strong donut game with iconic shops like Glazed and Infused and Do-Rite Donuts.\n"
          ]
        }
      ],
      "source": [
        "print(llm.invoke(prompt.format(food=\"donuts\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbelXktTkzRq"
      },
      "source": [
        "### Ланцюги: Поєднання LLM (великі мовні моделі) та запитів у багатоступеневих робочих процесах"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Організувтати ланцюг дуже просто:"
      ],
      "metadata": {
        "id": "rjQFU8f0YL4I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jyUSrYpknCV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7478137c-2567-445c-e18e-cecf930c84a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Tokyo, Japan: Known for its world-renowned sushi and sashimi, Tokyo is a must-visit destination for seafood lovers. The Tsukiji Fish Market, the largest fish market in the world, offers a wide variety of fresh fish and seafood.\n",
            "\n",
            "2. San Sebastian, Spain: This coastal city in the Basque Country is famous for its pintxos (small tapas-style dishes) and fresh seafood. The city's Old Town is filled with traditional seafood restaurants and markets.\n",
            "\n",
            "3. Cape Town, South Africa: With its location on the coast, Cape Town offers a diverse range of fresh seafood, including local specialties like snoek and crayfish. The city's vibrant food scene also includes a variety of seafood-focused restaurants.\n",
            "\n",
            "4. Sydney, Australia: As a coastal city, Sydney is known for its fresh seafood, particularly its famous Sydney rock oysters. The city's fish markets and waterfront restaurants offer a wide selection of fresh fish and seafood dishes.\n",
            "\n",
            "5. Vancouver, Canada: Located on the Pacific coast, Vancouver is a seafood lover's paradise. The city's diverse culinary scene includes a variety of seafood options, from traditional fish and chips to upscale seafood restaurants. The Granville Island Public Market is a must-visit for fresh seafood and\n"
          ]
        }
      ],
      "source": [
        "chain = prompt | llm\n",
        "print(chain.invoke(\"fresh fish\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQxNVxu9lU0S"
      },
      "source": [
        "### Агенти: Можливість динамічно викликати ланцюги на основі введення користувача\n",
        "\n",
        "**Агент**\n",
        "\n",
        "Як зазначено в документації, Агент в LangChain охоплює наступні абстракції:\n",
        "\n",
        "1. **AgentAction**: Представляє наступну дію, яку необхідно виконати, що складається з інструменту та вхідних даних для інструменту.\n",
        "   \n",
        "2. **AgentFinish**: Остаточний результат від агента, що містить фінальний результат агента в `return_values`.\n",
        "   \n",
        "3. **Intermediate Steps**: Означає попередні дії агента та їх відповідні результати, організовані як список кортежів `[AgentAction, Any]`.\n",
        "\n",
        "Ця структура вказує на те, що для запиту може бути виконано кілька дій агента за потреби, при цьому проміжні дії зберігаються в проміжних кроках.\n",
        "\n",
        "**AgentExecutor**\n",
        "\n",
        "AgentExecutor відповідає за використання Агента до тих пір, поки не буде отримано остаточний результат. Таким чином, він використовує Агента для отримання наступної дії, виконує повернуту дію поетапно та продовжує цей процес, доки не буде згенеровано остаточну відповідь для даного вводу.\n",
        "\n",
        "Раптом вам цікаов, як влаштовані технічно агенти під капотом:\n",
        "![](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*CD2Svi5BUZaG-d-KWeU3ug.png)\n",
        "\n",
        "Детально про те, як під капотом працюють агенти можна прочитати [тут](https://nakamasato.medium.com/langchain-how-an-agent-works-7dce1569933d)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3_j3IHYlXT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "803bad01-83af-4978-edd2-bff6a3d0a1d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q google-search-results langchain-community langchain_experimental"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "852Ucalcrgh7"
      },
      "source": [
        "Щоб зробити API пошуку доступним (100 пошуків на місяць), просто зареєструйтесь [тут](https://serpapi.com/users/welcome).\n",
        "Ми будемо використовувати [ReAct](https://react-lm.github.io/) агента. ReAct від Reasoning-Action. Агенти ReAct від LangChain допомагають організувати весь процес обробки запитів. Використовуючи ці агенти, ми можемо розбити складні запити на керовані кроки і виконувати їх систематично. Агент задається спеціальним промптом, який ми далі з вами розглянемо."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeVfjVALlUjl"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import load_tools\n",
        "from langchain.agents import Tool, AgentExecutor, AgentType, create_react_agent, initialize_agent\n",
        "from langchain.chains import LLMMathChain\n",
        "from langchain_experimental.utilities import PythonREPL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=overal_temperature)\n",
        "problem_chain = LLMMathChain.from_llm(llm=llm)\n",
        "math_tool = Tool.from_function(name=\"Calculator\",\n",
        "                func=problem_chain.run,\n",
        "                description=\"Useful for when you need to answer questions about math. This tool is only for math questions and nothing else. Only input math expressions. Cannot perfor max/min/prime number search operations.\")"
      ],
      "metadata": {
        "id": "hOVsID0Z5dNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "problem_chain.invoke('(112*132)-19/3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwWXeZr57519",
        "outputId": "5d38445a-afa4-4612-e353-7f6dd9d7d160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': '(112*132)-19/3', 'answer': 'Answer: 14777.666666666666'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke('(112*132)-19/3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUnH0xyCGeKy",
        "outputId": "50207d96-02c2-408d-f32d-3e9989edf119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='To solve the expression \\\\( (112 \\\\times 132) - \\\\frac{19}{3} \\\\), we first calculate \\\\( 112 \\\\times 132 \\\\):\\n\\n\\\\[\\n112 \\\\times 132 = 14784\\n\\\\]\\n\\nNext, we calculate \\\\( \\\\frac{19}{3} \\\\):\\n\\n\\\\[\\n\\\\frac{19}{3} \\\\approx 6.3333\\n\\\\]\\n\\nNow, we subtract \\\\( \\\\frac{19}{3} \\\\) from \\\\( 14784 \\\\):\\n\\n\\\\[\\n14784 - \\\\frac{19}{3} = 14784 - 6.3333 \\\\approx 14777.6667\\n\\\\]\\n\\nThus, the final result is approximately:\\n\\n\\\\[\\n14777.67\\n\\\\] \\n\\nIf you need the exact value in fractional form, it would be:\\n\\n\\\\[\\n14784 - \\\\frac{19}{3} = \\\\frac{44352 - 19}{3} = \\\\frac{44333}{3}\\n\\\\]\\n\\nSo, the final answer can be expressed as:\\n\\n\\\\[\\n\\\\frac{44333}{3} \\\\text{ or approximately } 14777.67\\n\\\\]', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 236, 'prompt_tokens': 15, 'total_tokens': 251, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CG8ik9m6ZvCJjDxi34ZeNEqRrVfgr', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--75a249ec-b634-4740-9873-6a2742ff0842-0', usage_metadata={'input_tokens': 15, 'output_tokens': 236, 'total_tokens': 251, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(112*132)-19/3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SFcOoQZGX_y",
        "outputId": "fb3d2f39-9d86-44f5-dfaf-701ac40c19ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14777.666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "REPL (Read-Eval-Print Loop) — це інтерактивне середовище програмування, яке дозволяє користувачу вводити код, отримувати його результат і продовжувати виконання в режимі реального часу. По суті, це цикл, який \"читає\" введення, \"оцінює\" його, \"друкує\" результат і чекає на новий введений код. REPL часто використовується в мовах програмування для швидкого тестування й налагодження коду, таких як Python, JavaScript тощо."
      ],
      "metadata": {
        "id": "nifd_Rxrpzih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python_repl = PythonREPL()\n",
        "python_tool = Tool(\n",
        "    name=\"python_repl\",\n",
        "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you necessarily should print it out with `print(...)`. Otherwise you won't see the result! It's very important.\",\n",
        "    func=python_repl.run,\n",
        ")\n",
        "python_tool.name = \"python_interpreter\""
      ],
      "metadata": {
        "id": "L1yo3LLV7qDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python_repl.run(\"import pandas as pd; s = pd.Series([1,2,3])\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8Lz9wnxp72Ro",
        "outputId": "0e397c03-5e8b-4309-fccf-02b01d039713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OepTDXlQmvvu"
      },
      "outputs": [],
      "source": [
        "os.environ[\"SERPAPI_API_KEY\"] = creds[\"SERPAPI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhrz11BSlhkl"
      },
      "outputs": [],
      "source": [
        "tools = load_tools([\"serpapi\"], llm=llm)\n",
        "prompt = hub.pull(\"hwchase17/react\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wc04KCgjHlkO",
        "outputId": "b6dc9d38-a71d-4c58-eed4-deabff0431b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Tool(name='Search', description='A search engine. Useful for when you need to answer questions about current events. Input should be a search query.', func=<bound method SerpAPIWrapper.run of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='5d906dfd010009d73a4a602a0ad8ac770f46dc2077bfd05ed5e4ffe4471ca2ef', aiosession=None)>, coroutine=<bound method SerpAPIWrapper.arun of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='5d906dfd010009d73a4a602a0ad8ac770f46dc2077bfd05ed5e4ffe4471ca2ef', aiosession=None)>)]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools.append(math_tool)\n",
        "tools.append(python_tool)"
      ],
      "metadata": {
        "id": "0yJ2_nMk6Xeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tools"
      ],
      "metadata": {
        "id": "jh_QVyGfHqjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjWtEgNB4SyK",
        "outputId": "042cdbab-59d4-42b8-a974-d9c0a2632dd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "{tools}\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [{tool_names}]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: {input}\n",
            "Thought:{agent_scratchpad}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo_2mVrSm3zV"
      },
      "outputs": [],
      "source": [
        "agent = create_react_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQQP-n52m9EJ"
      },
      "source": [
        "Дивіться список типів агентів [тут](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lklHRp6om-Q0",
        "outputId": "a4909817-e823-46c4-edc4-7e9717c26725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo answer the question, I first need to find out who the current leader of China is. After that, I will determine their age and find the largest prime number smaller than that age.\n",
            "\n",
            "Action: Search  \n",
            "Action Input: \"current leader of China 2023\"  \n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m['In October 2022, Xi secured a third term as CCP General Secretary, and was re-elected state president for an unprecedented third term in March 2023.', 'Xi Jinping is a politician and government official who became president of China in 2013 and general secretary of the Chinese Communist Party in 2012. He was ...', \"President of China ; Incumbent Xi Jinping. since 14 March 2013. Office of the President of the. People's Republic of China ; Incumbent Xi Jinping. since 14 March ...\", \"Xi Jinping · The leader of the world's most populated country and second largest economy (GDP), Xi Jinping holds a firm grasp on China's only political party.\", \"China's leader Xi Jinping has secured a historic third term as president from the country's rubber-stamp parliament.\", \"Xi Jinping is elected president of the People's Republic of China (PRC) and chairman of the PRC Central Military Commission.\", \"China's rubber-stamp parliament handed leader Xi Jinping a third term as president on Friday, an outcome that has been widely expected.\", \"New premier Li Qiang, 63, a close confidant of Chinese leader Xi Jinping, faces the task of reviving China's flagging economy.\", 'China President Xi Jinping hosts a welcome banquet in Beijing on Tuesday Oct 17, 2023. Chinese leader Xi Jinping addresses visiting dignitaries ...']\u001b[0m\u001b[32;1m\u001b[1;3mI have confirmed that the current leader of China is Xi Jinping. Next, I need to find out his age to determine the largest prime number smaller than that age.\n",
            "\n",
            "Action: Search  \n",
            "Action Input: \"Xi Jinping age 2023\"  \n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mAbout 70 years\u001b[0m\u001b[32;1m\u001b[1;3mTo find the exact age of Xi Jinping, I need to determine his birth year. He was born on June 15, 1953. Given that the current year is 2023, I can calculate his age.\n",
            "\n",
            "Action: python_interpreter  \n",
            "Action Input: `2023 - 1953`  \n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mThe calculation shows that Xi Jinping is 70 years old in 2023. Now, I need to find the largest prime number smaller than 70.\n",
            "\n",
            "Action: python_interpreter  \n",
            "Action Input: `max([x for x in range(2, 70) if all(x % i != 0 for i in range(2, int(x**0.5) + 1))])`  \n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mThe calculation for the largest prime number smaller than 70 has been completed. \n",
            "\n",
            "Final Answer: The current leader of China is Xi Jinping, and the largest prime number smaller than his age (70) is 67.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Who is the current leader of China? What is the largest prime number that is smaller than their age?',\n",
              " 'output': 'The current leader of China is Xi Jinping, and the largest prime number smaller than his age (70) is 67.'}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "agent_executor.invoke({'input': \"Who is the current leader of China? What is the largest prime number that is smaller than their age?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Агенти мають особиливий тип:"
      ],
      "metadata": {
        "id": "-8V-GYOigCl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(agent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "GWSaTSWZf-EH",
        "outputId": "f23af158-a10c-4569-e4eb-2c7a818cad13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.runnables.base.RunnableSequence"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.runnables.base.RunnableSequence</b><br/>def __init__(*steps: RunnableLike, name: Optional[str]=None, first: Optional[Runnable[Any, Any]]=None, middle: Optional[list[Runnable[Any, Any]]]=None, last: Optional[Runnable[Any, Any]]=None) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py</a>Sequence of ``Runnables``, where the output of each is the input of the next.\n",
              "\n",
              "**``RunnableSequence``** is the most important composition operator in LangChain\n",
              "as it is used in virtually every chain.\n",
              "\n",
              "A ``RunnableSequence`` can be instantiated directly or more commonly by using the\n",
              "``|`` operator where either the left or right operands (or both) must be a\n",
              "``Runnable``.\n",
              "\n",
              "Any ``RunnableSequence`` automatically supports sync, async, batch.\n",
              "\n",
              "The default implementations of ``batch`` and ``abatch`` utilize threadpools and\n",
              "asyncio gather and will be faster than naive invocation of ``invoke`` or ``ainvoke``\n",
              "for IO bound ``Runnable``s.\n",
              "\n",
              "Batching is implemented by invoking the batch method on each component of the\n",
              "``RunnableSequence`` in order.\n",
              "\n",
              "A ``RunnableSequence`` preserves the streaming properties of its components, so if\n",
              "all components of the sequence implement a ``transform`` method -- which\n",
              "is the method that implements the logic to map a streaming input to a streaming\n",
              "output -- then the sequence will be able to stream input to output!\n",
              "\n",
              "If any component of the sequence does not implement transform then the\n",
              "streaming will only begin after this component is run. If there are\n",
              "multiple blocking components, streaming begins after the last one.\n",
              "\n",
              ".. note::\n",
              "    ``RunnableLambdas`` do not support ``transform`` by default! So if you need to\n",
              "    use a ``RunnableLambdas`` be careful about where you place them in a\n",
              "    ``RunnableSequence`` (if you need to use the ``stream``/``astream`` methods).\n",
              "\n",
              "    If you need arbitrary logic and need streaming, you can subclass\n",
              "    Runnable, and implement ``transform`` for whatever logic you need.\n",
              "\n",
              "Here is a simple example that uses simple functions to illustrate the use of\n",
              "``RunnableSequence``:\n",
              "\n",
              "    .. code-block:: python\n",
              "\n",
              "        from langchain_core.runnables import RunnableLambda\n",
              "\n",
              "\n",
              "        def add_one(x: int) -&gt; int:\n",
              "            return x + 1\n",
              "\n",
              "\n",
              "        def mul_two(x: int) -&gt; int:\n",
              "            return x * 2\n",
              "\n",
              "\n",
              "        runnable_1 = RunnableLambda(add_one)\n",
              "        runnable_2 = RunnableLambda(mul_two)\n",
              "        sequence = runnable_1 | runnable_2\n",
              "        # Or equivalently:\n",
              "        # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
              "        sequence.invoke(1)\n",
              "        await sequence.ainvoke(1)\n",
              "\n",
              "        sequence.batch([1, 2, 3])\n",
              "        await sequence.abatch([1, 2, 3])\n",
              "\n",
              "Here&#x27;s an example that uses streams JSON output generated by an LLM:\n",
              "\n",
              "    .. code-block:: python\n",
              "\n",
              "        from langchain_core.output_parsers.json import SimpleJsonOutputParser\n",
              "        from langchain_openai import ChatOpenAI\n",
              "\n",
              "        prompt = PromptTemplate.from_template(\n",
              "            &quot;In JSON format, give me a list of {topic} and their &quot;\n",
              "            &quot;corresponding names in French, Spanish and in a &quot;\n",
              "            &quot;Cat Language.&quot;\n",
              "        )\n",
              "\n",
              "        model = ChatOpenAI()\n",
              "        chain = prompt | model | SimpleJsonOutputParser()\n",
              "\n",
              "        async for chunk in chain.astream({&quot;topic&quot;: &quot;colors&quot;}):\n",
              "            print(&quot;-&quot;)  # noqa: T201\n",
              "            print(chunk, sep=&quot;&quot;, flush=True)  # noqa: T201</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 2856);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Він створюється за допомогою RunnableAssign, PromptTemplate, RunnableBinding та ReActSingleInputOutputParser, структурованих у формі [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel)"
      ],
      "metadata": {
        "id": "uJiVsBKsg01o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent"
      ],
      "metadata": {
        "id": "oGUCf13cghhl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e053027e-2652-45ad-d282-c921a4c61153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RunnableAssign(mapper={\n",
              "  agent_scratchpad: RunnableLambda(lambda x: format_log_to_str(x['intermediate_steps']))\n",
              "})\n",
              "| PromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={}, partial_variables={'tools': \"Search(query: str, **kwargs: Any) -> str - A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\nCalculator(*args: Any, callbacks: Union[list[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, **kwargs: Any) -> Any - Useful for when you need to answer questions about math. This tool is only for math questions and nothing else. Only input math expressions. Cannot perfor max/min/prime number search operations.\\npython_interpreter(command: str, timeout: Optional[int] = None) -> str - A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you necessarily should print it out with `print(...)`. Otherwise you won't see the result! It's very important.\", 'tool_names': 'Search, Calculator, python_interpreter'}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'react', 'lc_hub_commit_hash': 'd15fe3c426f1c4b3f37c9198853e4a86e20c425ca7f4752ec0c9b0e97ca7ea4d'}, template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}')\n",
              "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7bb04cdc0410>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7bb0462b7830>, root_client=<openai.OpenAI object at 0x7bb0462b5670>, root_async_client=<openai.AsyncOpenAI object at 0x7bb0462b4080>, model_name='gpt-4o-mini', temperature=0.1, model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={'stop': ['\\nObservation']}, config={}, config_factories=[])\n",
              "| ReActSingleInputOutputParser()"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перший крок — це RunnableAssign, який відповідає за присвоєння пар \"ключ-значення\" вхідним даним формату Dict[str, Any]. У цьому випадку ключем є agent_scratchpad, а значенням є RunnableLambda, що перетворює intermediate_steps на рядок."
      ],
      "metadata": {
        "id": "sTCHfhigg-EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent.steps[1].partial_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_xmX2HPglr0",
        "outputId": "5adfec8c-474d-4847-d08b-23b1c7ccf1b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tools': \"Search(query: str, **kwargs: Any) -> str - A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\nCalculator(*args: Any, callbacks: Union[list[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, **kwargs: Any) -> Any - Useful for when you need to answer questions about math. This tool is only for math questions and nothing else. Only input math expressions. Cannot perfor max/min/prime number search operations.\\npython_interpreter(command: str, timeout: Optional[int] = None) -> str - A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you necessarily should print it out with `print(...)`. Otherwise you won't see the result! It's very important.\",\n",
              " 'tool_names': 'Search, Calculator, python_interpreter'}"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ми ще і промпти на кожному етапі можемо переглянути."
      ],
      "metadata": {
        "id": "IKOV10C7hwBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(agent.steps[1].template)"
      ],
      "metadata": {
        "id": "mvOS-xqGhuIU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb35b5e3-9a09-4229-da70-17331c76020c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the following questions as best you can. You have access to the following tools:\n",
            "\n",
            "{tools}\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Question: the input question you must answer\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, should be one of [{tool_names}]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\n",
            "Begin!\n",
            "\n",
            "Question: {input}\n",
            "Thought:{agent_scratchpad}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV7EY8hirPoR"
      },
      "source": [
        "### Пам'ять\n",
        "Часто нам хочеться, аби LLM памʼятала історію діалогу. Найпростіша форма пам'яті - це просто передача повідомлень історії чату по ланцюжку. Ось приклад:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "ai_msg = chain.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(\n",
        "                content=\"Translate from English to French: I love programming.\"\n",
        "            ),\n",
        "            AIMessage(content=\"J'adore la programmation.\"),\n",
        "            HumanMessage(content=\"What did you just say?\"),\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "print(ai_msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwH30dsplv1Z",
        "outputId": "50c00c47-1bce-4af1-e54e-a796b05ba0a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='I said \"J\\'adore la programmation,\" which translates to \"I love programming\" in French.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 56, 'total_tokens': 75, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CG8pwUhhAF5BPa9hzWiJuMypXJIFz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--9b1bdbcd-8928-466c-a3ba-656e1445635b-0' usage_metadata={'input_tokens': 56, 'output_tokens': 19, 'total_tokens': 75, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai_msg.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "A0lppeeUMNdk",
        "outputId": "4c8a71ee-0650-403b-c085-467a1c727763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I said \"J\\'adore la programmation,\" which translates to \"I love programming\" in French.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ми бачимо, що, передаючи попередню розмову по ланцюжку, він може використовувати її як контекст для відповідей на запитання. Це основна концепція, що лежить в основі пам'яті чат-ботів - решта посібника продемонструє зручні прийоми для передачі або переформатування повідомлень.\n"
      ],
      "metadata": {
        "id": "13zlMwv2n1zL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Автоматичне керування історією\n",
        "\n",
        "У попередніх прикладах повідомлення передаються в ланцюжок (та модель) явно. Це абсолютно прийнятний підхід, але він вимагає зовнішнього керування новими повідомленнями. LangChain також надає спосіб створення застосунків, що мають пам'ять, використовуючи [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) LangGraph. Ви можете увімкнути persistence у застосунках LangGraph, надавши контрольну точку (checkpointer) під час компіляції графа.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y9za9DEdnp7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q langgraph"
      ],
      "metadata": {
        "id": "OA0XYEuAm_3A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b35a5c83-17f1-490b-fa8e-95c7c59373ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m41.0/43.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m932.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "# Визначаємо функцію, яка викликає модель\n",
        "def call_model(state: MessagesState):\n",
        "    system_prompt = (\n",
        "        \"You are a helpful assistant. \"\n",
        "        \"Answer all questions to the best of your ability.\"\n",
        "    )\n",
        "    messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "# Визначаємо вузол та ребро\n",
        "workflow.add_node(\"model\", call_model)\n",
        "workflow.add_edge(START, \"model\")\n",
        "\n",
        "# Додаємо простий контроль стану в пам'яті\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "Lien7znQm6dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"Translate to French: I love programming.\")]},\n",
        "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZjnkTgYnN6i",
        "outputId": "18c39b98-325d-4b2d-f475-9721b9ca6cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='21ec9a11-0de2-4c97-9a81-9b8f06b8091c'),\n",
              "  AIMessage(content=\"J'adore programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_482c22a7bc', 'finish_reason': 'stop', 'logprobs': None}, id='run-7a7bd92b-63bc-4059-a542-9dc2668f7489-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"What did I just ask you?\")]},\n",
        "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWrLOQrwnVrs",
        "outputId": "28ec3d6f-0017-4b44-bea2-9c6114db69ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='21ec9a11-0de2-4c97-9a81-9b8f06b8091c'),\n",
              "  AIMessage(content=\"J'adore programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_482c22a7bc', 'finish_reason': 'stop', 'logprobs': None}, id='run-7a7bd92b-63bc-4059-a542-9dc2668f7489-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}),\n",
              "  HumanMessage(content='What did I just ask you?', additional_kwargs={}, response_metadata={}, id='01b3f193-5f40-4103-a639-093e4149ff1c'),\n",
              "  AIMessage(content='You asked me to translate \"I love programming\" into French.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 54, 'total_tokens': 67, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_482c22a7bc', 'finish_reason': 'stop', 'logprobs': None}, id='run-e4bf6029-1d77-4b19-848f-129d4ca827a8-0', usage_metadata={'input_tokens': 54, 'output_tokens': 13, 'total_tokens': 67, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Зміна історії чату\n",
        "Зміна збережених повідомлень чату може допомогти вашому чат-боту впоратися з різними ситуаціями.\n",
        "\n",
        "## Обрізання повідомлень\n",
        "LLM і чат-моделі мають обмежені контекстні вікна, і навіть якщо ви не перевищуєте ліміти, ви можете обмежити кількість відволікаючих чинників, з якими доводиться мати справу моделі. Одне з рішень - обрізати повідомлення історії перед тим, як передавати їх моделі. Давайте розглянемо приклад історії з додатком, який ми оголосили вище:\n"
      ],
      "metadata": {
        "id": "Om0RC09emcv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo_ephemeral_chat_history = [\n",
        "    HumanMessage(content=\"Hey there! I'm Nemo.\"),\n",
        "    AIMessage(content=\"Hello!\"),\n",
        "    HumanMessage(content=\"How are you today?\"),\n",
        "    AIMessage(content=\"Fine thanks!\"),\n",
        "]\n",
        "\n",
        "app.invoke(\n",
        "    {\n",
        "        \"messages\": demo_ephemeral_chat_history\n",
        "        + [HumanMessage(content=\"What's my name?\")]\n",
        "    },\n",
        "    config={\"configurable\": {\"thread_id\": \"2\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA6JPbF5od3W",
        "outputId": "fb70c981-b6ae-41ce-bc00-7371cf1dc5a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='e5054b65-d7ee-477a-9519-027e923206fb'),\n",
              "  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='0d22c145-cc6d-4cea-a492-3318001cf2c8'),\n",
              "  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='6e6ca81c-6c4e-4d8f-a5e2-9377459909bd'),\n",
              "  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='e71319c0-aefb-4858-bb33-91e1452289d1'),\n",
              "  HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='19ee0ae3-fc42-486c-863a-669701ac6c89'),\n",
              "  AIMessage(content='Your name is Nemo!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 63, 'total_tokens': 68, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_482c22a7bc', 'finish_reason': 'stop', 'logprobs': None}, id='run-16f9babc-a91b-45e8-9750-9527af311545-0', usage_metadata={'input_tokens': 63, 'output_tokens': 5, 'total_tokens': 68, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ми бачимо, що програма запам'ятовує попередньо завантажене ім'я.\n",
        "\n",
        "Але уявімо, що у нас дуже маленьке контекстне вікно, і ми хочемо зменшити кількість повідомлень, що передаються моделі, до 2 останніх. Ми можемо використати вбудовану утиліту trim_messages для відсікання повідомлень на основі їх кількості токенів до того, як вони досягнуть нашого запиту. У цьому випадку ми будемо вважати кожне повідомлення за 1 «токен» і залишимо лише два останніх повідомлення:\n",
        "\n"
      ],
      "metadata": {
        "id": "snHoQ3vUorK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import trim_messages\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "# Визначаємо функцію обрізання повідомлень\n",
        "# рахуємо кожне повідомлення як 1 \"токен\" (token_counter=len) і залишаємо лише останні два повідомлення\n",
        "trimmer = trim_messages(strategy=\"last\", max_tokens=2, token_counter=len)\n",
        "\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "# Визначаємо функцію, яка викликає модель\n",
        "def call_model(state: MessagesState):\n",
        "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
        "    system_prompt = (\n",
        "        \"You are a helpful assistant. \"\n",
        "        \"Answer all questions to the best of your ability.\"\n",
        "    )\n",
        "    messages = [SystemMessage(content=system_prompt)] + trimmed_messages\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "# Визначаємо вузол та ребро\n",
        "workflow.add_node(\"model\", call_model)\n",
        "workflow.add_edge(START, \"model\")\n",
        "\n",
        "# Додаємо простий контроль стану в пам'яті\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)\n"
      ],
      "metadata": {
        "id": "3kGUgd-bot7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app.invoke(\n",
        "    {\n",
        "        \"messages\": demo_ephemeral_chat_history\n",
        "        + [HumanMessage(content=\"What is my name?\")]\n",
        "    },\n",
        "    config={\"configurable\": {\"thread_id\": \"3\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_StTuT13o2VA",
        "outputId": "22574193-14a1-4070-8f22-f58e7cdb97cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='e5054b65-d7ee-477a-9519-027e923206fb'),\n",
              "  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='0d22c145-cc6d-4cea-a492-3318001cf2c8'),\n",
              "  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='6e6ca81c-6c4e-4d8f-a5e2-9377459909bd'),\n",
              "  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='e71319c0-aefb-4858-bb33-91e1452289d1'),\n",
              "  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='a45d3ebd-247d-4191-ae02-7cb01281aa71'),\n",
              "  AIMessage(content=\"I'm sorry, but I don't have access to personal information about users unless you provide it. If you'd like, you can tell me your name!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 39, 'total_tokens': 68, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_482c22a7bc', 'finish_reason': 'stop', 'logprobs': None}, id='run-1173537a-df50-401a-8955-b244f8a6631e-0', usage_metadata={'input_tokens': 39, 'output_tokens': 29, 'total_tokens': 68, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Саммарі історії повідомлень\n",
        "\n",
        "Ми можемо використовувати цей промпт і в інших випадках. Наприклад, ми можемо використовувати додатковий виклик LLM для створення резюме розмови перед викликом нашого додатку. Давайте відтворимо нашу історію чату:\n"
      ],
      "metadata": {
        "id": "Ao4sO6JdpXbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo_ephemeral_chat_history = [\n",
        "    HumanMessage(content=\"Hey there! I'm Nemo.\"),\n",
        "    AIMessage(content=\"Hello!\"),\n",
        "    HumanMessage(content=\"How are you today?\"),\n",
        "    AIMessage(content=\"Fine thanks!\"),\n",
        "]"
      ],
      "metadata": {
        "id": "kf38KDEspg7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, RemoveMessage\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "\n",
        "# Функція, яка викликає модель\n",
        "def call_model(state: MessagesState):\n",
        "    system_prompt = (\n",
        "        \"You are a helpful assistant. \"\n",
        "        \"Answer all questions to the best of your ability. \"\n",
        "        \"The provided chat history includes a summary of the earlier conversation.\"\n",
        "    )\n",
        "    system_message = SystemMessage(content=system_prompt)\n",
        "    message_history = state[\"messages\"][:-1]  # вилучаємо найостанніше введення\n",
        "    # Підсумовуємо повідомлення, якщо історія чату досягає певного розміру\n",
        "    if len(message_history) >= 4:\n",
        "        last_human_message = state[\"messages\"][-1]\n",
        "        # Викликаємо модель для створення підсумкових повідомлень розмови\n",
        "        summary_prompt = (\n",
        "            \"Distill the above chat messages into a single summary message. \"\n",
        "            \"Include as many specific details as you can.\"\n",
        "        )\n",
        "        summary_message = llm.invoke(\n",
        "            message_history + [HumanMessage(content=summary_prompt)]\n",
        "        )\n",
        "\n",
        "        # Видаляємо повідомлення, які більше не хочемо відображати\n",
        "        delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"]]\n",
        "        # Повторно додаємо повідомлення від користувача\n",
        "        human_message = HumanMessage(content=last_human_message.content)\n",
        "        # Викликаємо модель для обробки підсумкових повідомлень і відповіді\n",
        "        response = llm.invoke([system_message, summary_message, human_message])\n",
        "        message_updates = [summary_message, human_message, response] + delete_messages\n",
        "    else:\n",
        "        message_updates = llm.invoke([system_message] + state[\"messages\"])\n",
        "\n",
        "    return {\"messages\": message_updates}\n",
        "\n",
        "\n",
        "# Визначаємо вузол та ребро\n",
        "workflow.add_node(\"model\", call_model)\n",
        "workflow.add_edge(START, \"model\")\n",
        "\n",
        "# Додаємо простий контроль стану в пам'яті\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)\n"
      ],
      "metadata": {
        "id": "cWhEwvExpjAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app.invoke(\n",
        "    {\n",
        "        \"messages\": demo_ephemeral_chat_history\n",
        "        + [HumanMessage(\"What did I say my name was?\")]\n",
        "    },\n",
        "    config={\"configurable\": {\"thread_id\": \"4\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhvmCM0_ppdE",
        "outputId": "c212c32a-8439-4d21-a7dc-f17f53f822e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [AIMessage(content='Nemo greeted me with \"Hey there!\" and asked how I was doing, to which I responded that I was fine.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 60, 'total_tokens': 85, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_482c22a7bc', 'finish_reason': 'stop', 'logprobs': None}, id='run-df5ffc76-77c8-4a2a-923b-31edac843e88-0', usage_metadata={'input_tokens': 60, 'output_tokens': 25, 'total_tokens': 85, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}),\n",
              "  HumanMessage(content='What did I say my name was?', additional_kwargs={}, response_metadata={}, id='e160fac5-f4a2-4dca-92e2-54aa32fe8c52'),\n",
              "  AIMessage(content='You mentioned that your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 76, 'total_tokens': 84, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_482c22a7bc', 'finish_reason': 'stop', 'logprobs': None}, id='run-0585565c-906b-4056-a7c9-4d7294e44f79-0', usage_metadata={'input_tokens': 76, 'output_tokens': 8, 'total_tokens': 84, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Зауважте, що повторний запуск програми продовжить накопичення історії доти, доки вона не досягне вказаної кількості повідомлень (у нашому випадку - чотирьох). У цей момент ми створимо ще один звіт, згенерований з початкового звіту плюс нові повідомлення, і так далі.\n"
      ],
      "metadata": {
        "id": "YvdI579VpvOJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e--hMIfWIwsj"
      },
      "source": [
        "# Порівняння та оцінка LLM (Large Language Models)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Зручно мати механізм порівняти кілька LLM. Для порівняння використаємо моделі з HuggingFace. Всі моделі можна знайти [тут](https://huggingface.co/models)\n",
        "\n",
        "**Hugging Face** — це платформа та компанія, яка спеціалізується на штучному інтелекті, зокрема на роботі з мовними моделями та NLP (обробкою природної мови). Вони створили популярну бібліотеку **Transformers**, яка надає інструменти для роботи з передовими моделями штучного інтелекту, такими як GPT, BERT, T5, та інші. Hugging Face також має платформу для спільного використання моделей, де розробники можуть завантажувати, тестувати та використовувати попередньо навчені моделі для різних задач, таких як текстова класифікація, генерація тексту та переклад."
      ],
      "metadata": {
        "id": "OBF8wW_TslU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q langchain-huggingface"
      ],
      "metadata": {
        "id": "yCB5frdJp7G6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip freeze | grep huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKNLpb701toV",
        "outputId": "2d144cb3-16e8-45ed-bf52-46304adc51d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huggingface-hub==0.34.4\n",
            "langchain-huggingface==0.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNA4TsHpu6OM"
      },
      "outputs": [],
      "source": [
        "os.environ[\"HF_TOKEN\"] = creds['HUGGINGFACEHUB_API_TOKEN']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgesD0jrvDyG"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    max_new_tokens=512,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    typical_p=0.95,\n",
        "    temperature=0.01,\n",
        "    repetition_penalty=1.03,\n",
        "    huggingfacehub_api_token=\"my-api-key\",\n",
        "    task='conversational'\n",
        ")\n",
        "print(llm.invoke(\"What is Deep Learning?\", ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "xoxUiHkLzCRO",
        "outputId": "147ed4cc-fc40-4330-dc1c-d1a3fd59e2ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Model mistralai/Mistral-7B-Instruct-v0.2 is not supported for task text-generation and provider featherless-ai. Supported task: conversational.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2137669215.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conversational'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What is Deep Learning?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         return (\n\u001b[0;32m--> 390\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    391\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    788\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    998\u001b[0m                 )\n\u001b[1;32m    999\u001b[0m             ]\n\u001b[0;32m-> 1000\u001b[0;31m             return self._generate_helper(\n\u001b[0m\u001b[1;32m   1001\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m             output = (\n\u001b[0;32m--> 815\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    816\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1576\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1577\u001b[0m             text = (\n\u001b[0;32m-> 1578\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1579\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_huggingface/llms/huggingface_endpoint.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         response_text = self.client.text_generation(\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mtext_generation\u001b[0;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[1;32m   2374\u001b[0m         \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2375\u001b[0m         \u001b[0mprovider_helper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_provider_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2376\u001b[0;31m         request_parameters = provider_helper.prepare_request(\n\u001b[0m\u001b[1;32m   2377\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2378\u001b[0m             \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/inference/_providers/_common.py\u001b[0m in \u001b[0;36mprepare_request\u001b[0;34m(self, inputs, parameters, headers, model, api_key, extra_payload)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# mapped model from HF model ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mprovider_mapping_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_mapping_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# default HF headers + user headers (to customize in subclasses)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/inference/_providers/_common.py\u001b[0m in \u001b[0;36m_prepare_mapping_info\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprovider_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    160\u001b[0m                 \u001b[0;34mf\"Model {model} is not supported for task {self.task} and provider {self.provider}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0;34mf\"Supported task: {provider_mapping.task}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Model mistralai/Mistral-7B-Instruct-v0.2 is not supported for task text-generation and provider featherless-ai. Supported task: conversational."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Після запису лекції сталась зміна в бібліотеці HuggingFace і попередній варіант роботи з моделями не працює з  причин, описаних в коментарі тут: https://github.com/langchain-ai/langchain/issues/31434#issuecomment-2936308959\n",
        "\n",
        "Тому пропонується натомість використовувтаи моделі так:"
      ],
      "metadata": {
        "id": "96jbHmuN3KbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "import os\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "  repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "  huggingfacehub_api_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"),\n",
        ")\n",
        "\n",
        "chat_model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "  SystemMessagePromptTemplate.from_template(\"You're a helpful assistant.\"),\n",
        "  HumanMessagePromptTemplate.from_template(\"{user_question}\"),\n",
        "])\n",
        "\n",
        "chain = prompt | chat_model\n",
        "\n",
        "response = chain.invoke({\"user_question\": \"What happens when an unstoppable force meets an immovable object?\"})\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1SLpmCV3emK",
        "outputId": "1645aa99-7132-408a-ed12-86e34442ccb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " According to a common philosophical thought experiment, when an unstoppable force meets an immovable object, neither the force nor the object would be able to move. This leads to a paradox, as both cannot be stopped, yet they cannot move each other. However, it's important to note that this is a thought experiment and doesn't have a definitive answer in the physical world.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "З цікавого ще - gemma не має прописаних провайдерів, а mistral має."
      ],
      "metadata": {
        "id": "Oj5eR96C59VU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, ModelInfo\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\" # Specify the model ID\n",
        "\n",
        "model_info: ModelInfo = HfApi().model_info(\n",
        "    model_id, expand=[\"inferenceProviderMapping\"]\n",
        ")\n",
        "\n",
        "model_info.inference_provider_mapping"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqwZOsM15z_m",
        "outputId": "93b694b0-2e43-4b56-8458-c5a480f049bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[InferenceProviderMapping(provider='together', hf_model_id='mistralai/Mistral-7B-Instruct-v0.3', provider_id='mistralai/Mistral-7B-Instruct-v0.3', status='live', task='conversational', adapter=None, adapter_weights_path=None, type=None),\n",
              " InferenceProviderMapping(provider='novita', hf_model_id='mistralai/Mistral-7B-Instruct-v0.3', provider_id='mistralai/mistral-7b-instruct', status='live', task='conversational', adapter=None, adapter_weights_path=None, type=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"google/gemma-7b\" # Specify the model ID\n",
        "\n",
        "model_info: ModelInfo = HfApi().model_info(\n",
        "    model_id, expand=[\"inferenceProviderMapping\"]\n",
        ")\n",
        "\n",
        "model_info.inference_provider_mapping"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUjolpA66OS5",
        "outputId": "d2877a5f-ef93-4789-b17e-8e6b8ae3359c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тому будемо використовувати 2 інші модельки - llama та qwen."
      ],
      "metadata": {
        "id": "jg7hW1mp7pGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"meta-llama/Llama-3.1-8B-Instruct\" # Specify the model ID\n",
        "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "model_info: ModelInfo = HfApi().model_info(\n",
        "    model_id, expand=[\"inferenceProviderMapping\"]\n",
        ")\n",
        "\n",
        "model_info.inference_provider_mapping"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1d60cc4-e664-4343-eb54-2512c6d792cd",
        "id": "pxx3Z4Mq6729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[InferenceProviderMapping(provider='together', hf_model_id='Qwen/Qwen2.5-7B-Instruct', provider_id='Qwen/Qwen2.5-7B-Instruct-Turbo', status='live', task='conversational', adapter=None, adapter_weights_path=None, type=None),\n",
              " InferenceProviderMapping(provider='featherless-ai', hf_model_id='Qwen/Qwen2.5-7B-Instruct', provider_id='Qwen/Qwen2.5-7B-Instruct', status='live', task='conversational', adapter=None, adapter_weights_path=None, type=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# цей код наразі не працює\n",
        "\n",
        "# overal_temperature = 0.1\n",
        "\n",
        "# mistral = HuggingFaceEndpoint(\n",
        "#     repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "#     temperature=overal_temperature,\n",
        "#     max_new_tokens=200\n",
        "# )\n",
        "\n",
        "# gemma = HuggingFaceEndpoint(\n",
        "#     repo_id=\"google/gemma-7b\",\n",
        "#     temperature=overal_temperature,\n",
        "#     max_new_tokens=500\n",
        "# )"
      ],
      "metadata": {
        "id": "BvX1v6yAqUfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overal_temperature = 0.1\n",
        "\n",
        "mistral_llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    temperature=overal_temperature,\n",
        "    max_new_tokens=200\n",
        ")\n",
        "mistral = ChatHuggingFace(llm=mistral_llm)\n",
        "\n",
        "llama_llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    temperature=overal_temperature,\n",
        "    max_new_tokens=500\n",
        ")\n",
        "llama = ChatHuggingFace(llm=llama_llm)\n",
        "\n",
        "qwen_llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    temperature=overal_temperature,\n",
        "    max_new_tokens=500\n",
        ")\n",
        "qwen = ChatHuggingFace(llm=qwen_llm)"
      ],
      "metadata": {
        "id": "-pr-7xfs3zot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDWW8nDERcGU"
      },
      "source": [
        "## Налаштування лабораторії для порівняння"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy4s37W9m7X6"
      },
      "outputs": [],
      "source": [
        "from langchain.model_laboratory import ModelLaboratory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZUMGKuvn_HV"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5jHtwgkabw3"
      },
      "outputs": [],
      "source": [
        "models_list = [\n",
        "    mistral,\n",
        "    llama,\n",
        "    qwen\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s07AIuJm_Gv"
      },
      "outputs": [],
      "source": [
        "lab = ModelLaboratory.from_llms(models_list, prompt=prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDQ8VcedrkIw"
      },
      "source": [
        "Давайте запустимо це на деяких і порівняємо!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Eb6pNpimfu6",
        "outputId": "432ec222-31ff-4ea7-a4aa-a3f91b7a1310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "What is the opposite of up?\n",
            "\n",
            "llm=HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.2', max_new_tokens=200, temperature=0.1, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='mistralai/Mistral-7B-Instruct-v0.2', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>) model_id='mistralai/Mistral-7B-Instruct-v0.2' model_kwargs={}\n",
            "\u001b[36;1m\u001b[1;3m The opposite of \"up\" is typically considered to be \"down.\" When an object or a person moves from a higher position to a lower position, we often describe that movement as going \"down.\" Conversely, moving from a lower position to a higher position is described as going \"up.\" So, if you're looking for a word that is the polar opposite of \"up,\" then \"down\" is the answer you're looking for.\u001b[0m\n",
            "\n",
            "llm=HuggingFaceEndpoint(repo_id='meta-llama/Llama-3.1-8B-Instruct', max_new_tokens=500, temperature=0.1, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='meta-llama/Llama-3.1-8B-Instruct', client=<InferenceClient(model='meta-llama/Llama-3.1-8B-Instruct', timeout=120)>, async_client=<InferenceClient(model='meta-llama/Llama-3.1-8B-Instruct', timeout=120)>) model_id='meta-llama/Llama-3.1-8B-Instruct' model_kwargs={}\n",
            "\u001b[33;1m\u001b[1;3mThe opposite of \"up\" is \"down.\" These two directions are directly opposite each other.\u001b[0m\n",
            "\n",
            "llm=HuggingFaceEndpoint(repo_id='Qwen/Qwen2.5-7B-Instruct', max_new_tokens=500, temperature=0.1, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='Qwen/Qwen2.5-7B-Instruct', client=<InferenceClient(model='Qwen/Qwen2.5-7B-Instruct', timeout=120)>, async_client=<InferenceClient(model='Qwen/Qwen2.5-7B-Instruct', timeout=120)>) model_id='Qwen/Qwen2.5-7B-Instruct' model_kwargs={}\n",
            "\u001b[38;5;200m\u001b[1;3mThe opposite of \"up\" is \"down.\"\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lab.compare(\"What is the opposite of up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puWRd2nwT5eD",
        "outputId": "98e61e12-66f3-4a18-d403-7b2fc2dc88f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?\n",
            "\n",
            "llm=HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.2', max_new_tokens=200, temperature=0.1, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='mistralai/Mistral-7B-Instruct-v0.2', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>) model_id='mistralai/Mistral-7B-Instruct-v0.2' model_kwargs={}\n",
            "\u001b[36;1m\u001b[1;3m The cafeteria started with 23 apples. They used 20 apples for lunch, so that leaves us with:\n",
            "\n",
            "23 apples (initial amount) - 20 apples (used for lunch) = 3 apples remaining\n",
            "\n",
            "Then they bought an additional 6 apples. So the new total is:\n",
            "\n",
            "3 apples (remaining) + 6 apples (newly purchased) = 9 apples\n",
            "\n",
            "Therefore, the cafeteria has 9 apples.\u001b[0m\n",
            "\n",
            "llm=HuggingFaceEndpoint(repo_id='meta-llama/Llama-3.1-8B-Instruct', max_new_tokens=500, temperature=0.1, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='meta-llama/Llama-3.1-8B-Instruct', client=<InferenceClient(model='meta-llama/Llama-3.1-8B-Instruct', timeout=120)>, async_client=<InferenceClient(model='meta-llama/Llama-3.1-8B-Instruct', timeout=120)>) model_id='meta-llama/Llama-3.1-8B-Instruct' model_kwargs={}\n",
            "\u001b[33;1m\u001b[1;3mTo find the total number of apples, we need to consider the changes in the number of apples step by step:\n",
            "\n",
            "1. Initially, the cafeteria had 23 apples.\n",
            "2. They used 20 apples for lunch, so now they have:\n",
            "   23 (initial apples) - 20 (used for lunch) = 3 apples left.\n",
            "3. Then, they bought 6 more apples, so now they have:\n",
            "   3 (apples left) + 6 (apples bought) = 9 apples.\n",
            "\n",
            "So, the cafeteria now has 9 apples in total.\u001b[0m\n",
            "\n",
            "llm=HuggingFaceEndpoint(repo_id='Qwen/Qwen2.5-7B-Instruct', max_new_tokens=500, temperature=0.1, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='Qwen/Qwen2.5-7B-Instruct', client=<InferenceClient(model='Qwen/Qwen2.5-7B-Instruct', timeout=120)>, async_client=<InferenceClient(model='Qwen/Qwen2.5-7B-Instruct', timeout=120)>) model_id='Qwen/Qwen2.5-7B-Instruct' model_kwargs={}\n",
            "\u001b[38;5;200m\u001b[1;3mLet's break down the problem step by step:\n",
            "\n",
            "1. **Initial number of apples**: The cafeteria started with 23 apples.\n",
            "2. **Apples used for lunch**: They used 20 apples for lunch. To find out how many apples are left after using 20, we subtract 20 from 23:\n",
            "   \\[\n",
            "   23 - 20 = 3\n",
            "   \\]\n",
            "   So, after using 20 apples, they have 3 apples left.\n",
            "3. **Apples bought**: They then bought 6 more apples. To find out the total number of apples now, we add the 6 apples to the 3 apples they had left:\n",
            "   \\[\n",
            "   3 + 6 = 9\n",
            "   \\]\n",
            "\n",
            "Therefore, the cafeteria now has 9 apples.\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lab.compare(\"Answer the following question by reasoning step by step. The cafeteria had 23 apples. \\\n",
        "If they used 20 for lunch, and bought 6 more, how many apple do they have?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AP63DmDbaY_X",
        "outputId": "8ffede66-99a0-4643-bcca-6ec6432490db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "\n",
            "  Can Elon Musk have a conversation with George Washington? Give the rationale before answering.\n",
            "\n",
            "\n",
            "llm=HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.2', max_new_tokens=200, temperature=0.1, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='mistralai/Mistral-7B-Instruct-v0.2', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>) model_id='mistralai/Mistral-7B-Instruct-v0.2' model_kwargs={}\n",
            "\u001b[36;1m\u001b[1;3m I'd be happy to help answer your question, but it's important to note that Elon Musk and George Washington were historical figures who lived in different time periods. Elon Musk is a business magnate, industrial designer, and engineer who was born in 1971, while George Washington was a military leader, statesman, and the first President of the United States, who was born in 1732.\n",
            "\n",
            "It's not possible for Elon Musk to have a conversation with George Washington directly, as they existed at different points in history. However, we can imagine what a conversation between the two might be like based on their respective backgrounds and achievements. Such a conversation could potentially provide interesting insights into leadership, innovation, and the intersection of technology and society.\n",
            "\n",
            "That being said, any hypothetical conversation between Elon Musk and George Washington would be purely speculative and based on our current understanding of their respective historical contexts and personalities. It would not represent an actual interaction between the two individuals.\u001b[0m\n",
            "\n",
            "llm=HuggingFaceEndpoint(repo_id='meta-llama/Llama-3.1-8B-Instruct', max_new_tokens=500, temperature=0.1, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='meta-llama/Llama-3.1-8B-Instruct', client=<InferenceClient(model='meta-llama/Llama-3.1-8B-Instruct', timeout=120)>, async_client=<InferenceClient(model='meta-llama/Llama-3.1-8B-Instruct', timeout=120)>) model_id='meta-llama/Llama-3.1-8B-Instruct' model_kwargs={}\n",
            "\u001b[33;1m\u001b[1;3mWhile Elon Musk as a historical person is not feasible, and George Washington has passed away, I can consider an interesting alternative. If we're referring to a conversation between modern-day Elon Musk and the historical figure of George Washington in a hypothetical timeframe, it's quite possible.\n",
            "\n",
            "Consider a thought experiment where we have a time machine or a fictional device that allows George Washington to be temporarily in the present day. In such a scenario, it wouldn't be impossible for Elon Musk to have a conversation with George Washington.\n",
            "\n",
            "However, if we consider the context of such an encounter, it could become increasingly difficult to justify due to the vastly different historical, social, and technological disparities between the 18th and 21st centuries. Washington's understanding of modern concepts and technological advancements might not be thorough, and language barriers (due to differences in slang, idioms, and cultural references) could make communication somewhat challenging.\n",
            "\n",
            "Assuming the scenario allows for clear communication and historical accuracy, I can engage in the conversation.\n",
            "\n",
            "Would you like to continue with a conversation between Elon Musk and George Washington, or would you like me to provide more insights into their hypothetical meeting?\u001b[0m\n",
            "\n",
            "llm=HuggingFaceEndpoint(repo_id='Qwen/Qwen2.5-7B-Instruct', max_new_tokens=500, temperature=0.1, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='Qwen/Qwen2.5-7B-Instruct', client=<InferenceClient(model='Qwen/Qwen2.5-7B-Instruct', timeout=120)>, async_client=<InferenceClient(model='Qwen/Qwen2.5-7B-Instruct', timeout=120)>) model_id='Qwen/Qwen2.5-7B-Instruct' model_kwargs={}\n",
            "\u001b[38;5;200m\u001b[1;3mTo determine if Elon Musk can have a conversation with George Washington, we need to consider the time periods in which these individuals lived. \n",
            "\n",
            "1. **Time Periods**:\n",
            "   - **George Washington** (1732-1799): He lived during the 18th century and was a key figure in the American Revolutionary War and the early years of the United States.\n",
            "   - **Elon Musk** (born 1971): He is a contemporary figure, living in the 21st century and known for his work in technology, space exploration, and transportation.\n",
            "\n",
            "2. **Historical Context**:\n",
            "   - George Washington died in 1799, long before Elon Musk was born.\n",
            "   - Elon Musk was born in 1971, long after George Washington's death.\n",
            "\n",
            "Given these facts, it is impossible for Elon Musk to have a conversation with George Washington because they lived in different time periods and never coexisted. \n",
            "\n",
            "**Conclusion**: No, Elon Musk cannot have a conversation with George Washington.\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lab.compare('''\n",
        "  Can Elon Musk have a conversation with George Washington? Give the rationale before answering.\n",
        "''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbqm52kHarID"
      },
      "source": [
        "Давайте змінимо запит."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uoq5C6eMT5jr"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"You are a professional social media manager who can write great posts in linkedin to increase appeal of persons profile: {request}\n",
        "\n",
        "Story:\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"request\"])\n",
        "\n",
        "lab = ModelLaboratory.from_llms(models_list, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo1mo_8OT5nI",
        "outputId": "706c4b9b-86ae-4e74-eaa0-99ea083a59fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "I have passed a course in large language models (1 month duration). Write a post about that.\n",
            "\n",
            "llm=HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.2', max_new_tokens=200, temperature=0.1, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='mistralai/Mistral-7B-Instruct-v0.2', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>) model_id='mistralai/Mistral-7B-Instruct-v0.2' model_kwargs={}\n",
            "\u001b[36;1m\u001b[1;3m 🚀 Exciting News: I'm thrilled to announce that I've recently completed a 1-month intensive course in Large Language Models! 🔬📚\n",
            "\n",
            "If you're not familiar, Large Language Models are sophisticated AI systems designed to understand and generate human-like text. They're the technology behind many advanced natural language processing applications, including chatbots, translation tools, and writing assistants.\n",
            "\n",
            "Throughout the course, I had the privilege of learning from industry experts and gaining hands-on experience applying these models to various real-world projects. I explored techniques for training and fine-tuning large language models for specific tasks, including sentiment analysis, text generation, and question answering.\n",
            "\n",
            "🎯 My goal in completing this course was to expand my skillset and better understand how Large Language Models can be used to create more effective and engaging content in the professional world, particularly on LinkedIn. I'm excited to bring this new knowledge to clients and colleagues and look forward to exploring new ways to utilize this technology.\n",
            "\n",
            "💡 I believe that the potential applications for Large Language Models in the professional realm are vast and can lead to significant improvements in communication, marketing, and more. I'm eager to continue learning and exploring this exciting field, and I'd love to connect with others who share my interest and passion for technology, language, and innovation.\n",
            "\n",
            "If you've used Large Language Models in your own work or have any thoughts or questions about this technology, please share in the comments below or reach out to me directly. I'd be happy to connect and discuss! 💬🤝️\n",
            "\n",
            "#LargeLanguageModels #AI #MachineLearning #ProfessionalNetwork #LinkedIn #Technology #Innovation #Learning #PersonalDevelopment #Connections #NewSkills #Chatbots #NLP #TextGeneration #QuestionAnswering #Communication #Marketing #Business #TechTips #AIforBusiness #AIinBusiness #InnovationInAction\u001b[0m\n",
            "\n",
            "llm=HuggingFaceEndpoint(repo_id='meta-llama/Llama-3.1-8B-Instruct', max_new_tokens=500, temperature=0.1, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='meta-llama/Llama-3.1-8B-Instruct', client=<InferenceClient(model='meta-llama/Llama-3.1-8B-Instruct', timeout=120)>, async_client=<InferenceClient(model='meta-llama/Llama-3.1-8B-Instruct', timeout=120)>) model_id='meta-llama/Llama-3.1-8B-Instruct' model_kwargs={}\n",
            "\u001b[33;1m\u001b[1;3mHere's a possible LinkedIn post:\n",
            "\n",
            "**Just Levelled Up: Unlocking the Power of Large Language Models**\n",
            "\n",
            "I'm thrilled to share that I've recently completed a comprehensive course on Large Language Models (LLMs), and I'm still buzzing with excitement!\n",
            "\n",
            "Over the past month, I dove head-first into the world of LLMs, exploring the possibilities and limitations of these powerful AI engines. From conversational interfaces to content generation, I gained a deep understanding of how LLMs are revolutionizing the way we interact, create, and communicate.\n",
            "\n",
            "Some key takeaways from the course:\n",
            "\n",
            " Learning how to train, fine-tune, and deploy LLMs for real-world applications\n",
            " Mastering techniques for evaluating and interpreting LLM performance\n",
            " Exploring the intersection of NLP, computer vision, and multimodal interfaces\n",
            " Understanding the societal and ethical implications of LLM adoption\n",
            "\n",
            "With this new knowledge, I'm looking forward to applying the principles of LLMs to:\n",
            "\n",
            " Enhancing customer experience through more intuitive and informed chatbots\n",
            " Developing innovative solutions for content creation and curation\n",
            " Exploring new frontiers in human-AI collaboration\n",
            "\n",
            "If you're interested in staying ahead of the curve on AI and LLMs, I'd love to connect and share my insights and experiences. Let's chat about how we can harness the power of LLMs to drive business growth and societal progress!\n",
            "\n",
            "**Drop a comment below if you have any questions or would like to discuss further!**\n",
            "\n",
            "#LargeLanguageModels #ArtificialIntelligence #NaturalLanguageProcessing #AI #MachineLearning #Tech #Innovation #DigitalTransformation\u001b[0m\n",
            "\n",
            "llm=HuggingFaceEndpoint(repo_id='Qwen/Qwen2.5-7B-Instruct', max_new_tokens=500, temperature=0.1, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='Qwen/Qwen2.5-7B-Instruct', client=<InferenceClient(model='Qwen/Qwen2.5-7B-Instruct', timeout=120)>, async_client=<InferenceClient(model='Qwen/Qwen2.5-7B-Instruct', timeout=120)>) model_id='Qwen/Qwen2.5-7B-Instruct' model_kwargs={}\n",
            "\u001b[38;5;200m\u001b[1;3mCertainly! Here’s a LinkedIn post that highlights your achievement in completing a course on large language models, tailored to engage and inspire your network:\n",
            "\n",
            "---\n",
            "\n",
            "🌟 **A Month of Learning: Mastering Large Language Models** 🌟\n",
            "\n",
            "Hey everyone! 🌟\n",
            "\n",
            "I'm thrilled to share that I've just completed a month-long course on Large Language Models (LLMs)! 🚀 This intensive journey has been nothing short of transformative, and I'm excited to dive into the world of AI and natural language processing.\n",
            "\n",
            "During this course, I delved deep into the intricacies of LLMs, from understanding their architecture to exploring their applications in various industries. I've gained valuable insights into how these models are shaping the future of communication and innovation.\n",
            "\n",
            "What I've learned is not just theoretical; it's practical and applicable. I'm now better equipped to understand the nuances of language and how to leverage LLMs to enhance my work and projects. Whether it's improving customer service through chatbots, enhancing content creation, or even developing more sophisticated AI tools, the possibilities are endless.\n",
            "\n",
            "I'm eager to share my knowledge and experiences with you all. If you're curious about the latest in AI or want to learn more about how LLMs can benefit your work, feel free to reach out! Let's connect and explore the exciting world of large language models together.\n",
            "\n",
            "#AI #LargeLanguageModels #MachineLearning #TechTrends #ProfessionalDevelopment\n",
            "\n",
            "---\n",
            "\n",
            "This post is designed to be engaging and informative, highlighting your achievement while also inviting interaction and discussion within your network.\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lab.compare('''I have passed a course in large language models (1 month duration). Write a post about that.''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2c6EByDT5rQ"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Answer the question to the best of your abilities but if you are not sure then answer you don't know: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "lab = ModelLaboratory.from_llms(models_list, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qCuIkZST5uK",
        "outputId": "e951c248-6999-4ce4-f59a-258b417bd2e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "I am riding a bicycle. The pedals are moving fast. I look into the mirror and I am not moving. Why is this?\n",
            "\n",
            "llm=HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.2', max_new_tokens=200, temperature=0.1, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='mistralai/Mistral-7B-Instruct-v0.2', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>) model_id='mistralai/Mistral-7B-Instruct-v0.2' model_kwargs={}\n",
            "\u001b[36;1m\u001b[1;3m This situation could be explaining an optical illusion, often referred to as the \"Mirror Riddle.\" The reason why you may not be moving in the mirror while you're pedaling your bicycle is because the reflection in the mirror shows a stationary image of you pedaling, but in reality, you are continually moving forward with the bicycle. This visual discrepancy can be quite disorienting but does not mean that something is physically preventing you from moving. Explanation: As you pedal, your bicycle moves forward. However, when you look into the mirror, what you see is a reflection of you pedaling in place while your bicycle and surroundings remain stationary in the reflection. This illusion can be quite intriguing and emphasizes the importance of considering both perspective and the physical environment to understand complex situations.\u001b[0m\n",
            "\n",
            "llm=HuggingFaceEndpoint(repo_id='meta-llama/Llama-3.1-8B-Instruct', max_new_tokens=500, temperature=0.1, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='meta-llama/Llama-3.1-8B-Instruct', client=<InferenceClient(model='meta-llama/Llama-3.1-8B-Instruct', timeout=120)>, async_client=<InferenceClient(model='meta-llama/Llama-3.1-8B-Instruct', timeout=120)>) model_id='meta-llama/Llama-3.1-8B-Instruct' model_kwargs={}\n",
            "\u001b[33;1m\u001b[1;3mThis phenomenon is related to how we perceive motion in the mirror. When you look in the mirror, you are seeing yourself from behind, but you can't see the motion of the bike directly behind you. \n",
            "\n",
            "The reason you appear stationary in the mirror is because the mirror reflects your position at a given moment and the motion of your body is not clearly visible to you from that angle.\n",
            "\n",
            "Additionally, the fact that the pedals are moving fast is irrelevant in this context because your own motion is not visible to you in the mirror, only the motion of any objects in front of you would be visible.\n",
            "\n",
            "However, if you were to look in the mirror from the side, you would likely see both your legs moving quickly as you pedal the bike. \n",
            "\n",
            "This effect is often referred to as a \"parallax illusion,\" where the apparent motion of an object is influenced by the position and motion of the observer. In this case, the motion of your body and the bike is not directly visible to you in the mirror, creating the illusion that you are not moving. \n",
            "\n",
            "This effect is quite normal and occurs due to the way our brains process visual information.\u001b[0m\n",
            "\n",
            "llm=HuggingFaceEndpoint(repo_id='Qwen/Qwen2.5-7B-Instruct', max_new_tokens=500, temperature=0.1, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='Qwen/Qwen2.5-7B-Instruct', client=<InferenceClient(model='Qwen/Qwen2.5-7B-Instruct', timeout=120)>, async_client=<InferenceClient(model='Qwen/Qwen2.5-7B-Instruct', timeout=120)>) model_id='Qwen/Qwen2.5-7B-Instruct' model_kwargs={}\n",
            "\u001b[38;5;200m\u001b[1;3mThis situation can occur if you are riding on a stationary bike or a recumbent bicycle that has a mechanism to keep the pedals moving while you are not actually propelling the bike forward. In a stationary bike, the resistance and motion of the pedals are driven by a motor or flywheel, allowing you to pedal without the bike moving. In a recumbent bicycle, there might be a similar mechanism to keep the pedals in motion while the bike remains stationary.\n",
            "\n",
            "If you are using a regular bicycle and looking into a mirror, it's less likely that you would not be moving, unless you are in a very still position or the mirror is positioned in such a way that it reflects the motion of the pedals but not your forward movement. However, if you are stationary on a bike with a mechanism to keep the pedals moving, this would explain why you are not moving forward even though the pedals appear to be moving.\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lab.compare('''I am riding a bicycle. The pedals are moving fast. I look into the mirror and I am not moving. Why is this?''')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOks9w5ndiqu"
      },
      "source": [
        "### Визначення іменованих сутностей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JydfVsh8eAFx"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"{question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "lab = ModelLaboratory.from_llms(models_list, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lab.compare('''Extract names and cities from the text.\\n\\n\n",
        "Output in the format: {\"names\": list of names in text, \"cities\": list of cities in text}, surname]\\n\\n\n",
        "\n",
        "Mark Dickey, 40, began suffering from severe gastric pain last week after descending\n",
        "more than 3,600 feet into Morca Cave in Southern Turkey, according to a statement\n",
        "from the European Cave Rescue Association, but he wasn’t able to be rescued until yesterday,\n",
        "when doctors deemed him “transportable.”\n",
        "''')"
      ],
      "metadata": {
        "id": "L9mosc63-NN1",
        "outputId": "8a7524c3-9117-4577-abb1-301ca2004b69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "Extract names and cities from the text.\n",
            "\n",
            "\n",
            "Output in the format: {\"names\": list of names in text, \"cities\": list of cities in text}, surname]\n",
            "\n",
            "\n",
            "\n",
            "Mark Dickey, 40, began suffering from severe gastric pain last week after descending\n",
            "more than 3,600 feet into Morca Cave in Southern Turkey, according to a statement\n",
            "from the European Cave Rescue Association, but he wasn’t able to be rescued until yesterday,\n",
            "when doctors deemed him “transportable.”\n",
            "\n",
            "\n",
            "llm=HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.2', max_new_tokens=200, temperature=0.1, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='mistralai/Mistral-7B-Instruct-v0.2', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>) model_id='mistralai/Mistral-7B-Instruct-v0.2' model_kwargs={}\n",
            "\u001b[36;1m\u001b[1;3m {\n",
            "\"names\": [\"Mark Dickey\"],\n",
            "\"cities\": [\"Southern Turkey\"]\n",
            "}\u001b[0m\n",
            "\n",
            "llm=HuggingFaceEndpoint(repo_id='meta-llama/Llama-3.1-8B-Instruct', max_new_tokens=500, temperature=0.1, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='meta-llama/Llama-3.1-8B-Instruct', client=<InferenceClient(model='meta-llama/Llama-3.1-8B-Instruct', timeout=120)>, async_client=<InferenceClient(model='meta-llama/Llama-3.1-8B-Instruct', timeout=120)>) model_id='meta-llama/Llama-3.1-8B-Instruct' model_kwargs={}\n",
            "\u001b[33;1m\u001b[1;3m{\n",
            "  \"names\": [\"Mark Dickey\"],\n",
            "  \"cities\": [\"Southern Turkey\", \"Morca\"]\n",
            "}\u001b[0m\n",
            "\n",
            "llm=HuggingFaceEndpoint(repo_id='Qwen/Qwen2.5-7B-Instruct', max_new_tokens=500, temperature=0.1, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='Qwen/Qwen2.5-7B-Instruct', client=<InferenceClient(model='Qwen/Qwen2.5-7B-Instruct', timeout=120)>, async_client=<InferenceClient(model='Qwen/Qwen2.5-7B-Instruct', timeout=120)>) model_id='Qwen/Qwen2.5-7B-Instruct' model_kwargs={}\n",
            "\u001b[38;5;200m\u001b[1;3m{\"names\": [\"Mark Dickey\"], \"cities\": []}, Dickey\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQjad_C-dgDy"
      },
      "source": [
        "Відповіді на запитання на основі тексту"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lab.compare('''Is Mark Dickey alive?\\n\\n\n",
        "Output in the format: Yes or No, facts that prove that.\\n\\n\n",
        "\n",
        "Mark Dickey, 40, began suffering from severe gastric pain last week after descending\n",
        "more than 3,600 feet into Morca Cave in Southern Turkey, according to a statement\n",
        "from the European Cave Rescue Association, but he wasn’t able to be rescued until yesterday,\n",
        "when doctors deemed him “transportable.”\n",
        "''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YrzD_S4-H0K",
        "outputId": "056eab21-915b-4b3d-d484-13aca15807ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "Is Mark Dickey alive?\n",
            "\n",
            "\n",
            "Output in the format: Yes or No, facts that prove that.\n",
            "\n",
            "\n",
            "\n",
            "Mark Dickey, 40, began suffering from severe gastric pain last week after descending\n",
            "more than 3,600 feet into Morca Cave in Southern Turkey, according to a statement\n",
            "from the European Cave Rescue Association, but he wasn’t able to be rescued until yesterday,\n",
            "when doctors deemed him “transportable.”\n",
            "\n",
            "\n",
            "llm=HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.2', max_new_tokens=200, temperature=0.1, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='mistralai/Mistral-7B-Instruct-v0.2', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>) model_id='mistralai/Mistral-7B-Instruct-v0.2' model_kwargs={}\n",
            "\u001b[36;1m\u001b[1;3m Yes, Mark Dickey is alive based on the information provided in the statement from the European Cave Rescue Association. The statement indicates that Mark was suffering from severe gastric pain and wasn't able to be rescued until yesterday when doctors deemed him \"transportable,\" implying that he is still alive and being rescued.\u001b[0m\n",
            "\n",
            "llm=HuggingFaceEndpoint(repo_id='meta-llama/Llama-3.1-8B-Instruct', max_new_tokens=500, temperature=0.1, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='meta-llama/Llama-3.1-8B-Instruct', client=<InferenceClient(model='meta-llama/Llama-3.1-8B-Instruct', timeout=120)>, async_client=<InferenceClient(model='meta-llama/Llama-3.1-8B-Instruct', timeout=120)>) model_id='meta-llama/Llama-3.1-8B-Instruct' model_kwargs={}\n",
            "\u001b[33;1m\u001b[1;3mNo\n",
            "\n",
            "Reasons that prove Mark Dickey is not alive:\n",
            "\n",
            "- He was not able to be rescued immediately and had to wait.\n",
            "- Doctors deemed him \"transportable,\" implying that he was previously unable to move or be moved.\n",
            "- He had been suffering from severe gastric pain last week, which implies that the situation had become critical.\u001b[0m\n",
            "\n",
            "llm=HuggingFaceEndpoint(repo_id='Qwen/Qwen2.5-7B-Instruct', max_new_tokens=500, temperature=0.1, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='Qwen/Qwen2.5-7B-Instruct', client=<InferenceClient(model='Qwen/Qwen2.5-7B-Instruct', timeout=120)>, async_client=<InferenceClient(model='Qwen/Qwen2.5-7B-Instruct', timeout=120)>) model_id='Qwen/Qwen2.5-7B-Instruct' model_kwargs={}\n",
            "\u001b[38;5;200m\u001b[1;3mYes, facts that prove this are:\n",
            "\n",
            "- Mark Dickey was rescued yesterday.\n",
            "- Doctors deemed him \"transportable,\" indicating he is alive and being treated.\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX9NagGxtZyv"
      },
      "source": [
        "Більше прикладів - в репозиторії LangChain."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}